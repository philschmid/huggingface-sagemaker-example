{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker example using `Trainer` class\n",
    "\n",
    "Each folder starting with `0X_..` contains an specific sagemaker example. Each example contains a jupyter notebooke `sagemaker-example.ipynb` and a `src/` folder. The `sagemaker-example` is a jupyter notebook which is used to train transformers and datasets on AWS Sagemaker. The `src/` folder contains the `train.py`, our training script and `requirements.txt` for additional dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Sagemaker Session with local AWS Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_profile_name='hf-sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# creates a boto3 session using the local profile we defined\n",
    "bt3 = boto3.session.Session(profile_name=local_profile_name)\n",
    "\n",
    "\n",
    "sess = sagemaker.Session(boto_session=bt3)\n",
    "\n",
    "# since we are using the sagemaker-sdk locally we cannot `get_execution_role` \n",
    "# role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From outside these notebooks, `get_execution_role()` will return an exception because it does not know what is the role name that SageMaker requires.\n",
    "\n",
    "To solve this issue, pass the IAM role name instead of using `get_execution_role()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = \"SageMakerRole\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_WARNING: This policy gives full S3 access to the container that is running in SageMaker. You can change this policy to a more restrictive one, or create your own policy._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name SageMakerRole already exists.\n"
     ]
    }
   ],
   "source": [
    "%%bash  -s \"$local_profile_name\" \"$role_name\" \n",
    "# This script creates a role named SageMakerRole\n",
    "# that can be used by SageMaker and has Full access to S3.\n",
    "\n",
    "ROLE_NAME=$2\n",
    "\n",
    "# WARNING: this policy gives full S3 access to container that\n",
    "# is running in SageMaker. You can change this policy to a more\n",
    "# restrictive one, or create your own policy.\n",
    "POLICY_S3=arn:aws:iam::aws:policy/AmazonS3FullAccess\n",
    "\n",
    "# Creates a AWS policy that allows the role to interact\n",
    "# with ANY S3 bucket\n",
    "cat <<EOF > /tmp/assume-role-policy-document.json\n",
    "{\n",
    "\t\"Version\": \"2012-10-17\",\n",
    "\t\"Statement\": [{\n",
    "\t\t\"Effect\": \"Allow\",\n",
    "\t\t\"Principal\": {\n",
    "\t\t\t\"Service\": \"sagemaker.amazonaws.com\"\n",
    "\t\t},\n",
    "\t\t\"Action\": \"sts:AssumeRole\"\n",
    "\t}]\n",
    "}\n",
    "EOF\n",
    "\n",
    "# Creates the role\n",
    "aws iam create-role --profile $1  --role-name ${ROLE_NAME} --assume-role-policy-document file:///tmp/assume-role-policy-document.json\n",
    "\n",
    "# attaches the S3 full access policy to the role\n",
    "aws iam attach-role-policy --profile $1 --policy-arn ${POLICY_S3}  --role-name ${ROLE_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get create role arn \n",
    "iam = bt3.client('iam')\n",
    "role = iam.get_role(RoleName=role_name)['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an local estimator for testing\n",
    "\n",
    "You run PyTorch training scripts on SageMaker by creating PyTorch Estimators. SageMaker training of your script is invoked when you call fit on a PyTorch Estimator. The following code sample shows how you train a custom PyTorch script `train.py`, passing in three hyperparameters (`epochs`). We are not going to pass any data into sagemaker training job instead it will be downloaded in `train.py`\n",
    "\n",
    "in sagemaker you can test you training in a \"local-mode\" by setting your instance_type to `'local'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "pytorch_estimator = PyTorch(entry_point='train.py',\n",
    "                            source_dir='src',\n",
    "                            base_job_name='huggingface',\n",
    "                            instance_type='local',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version='1.5.0',\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmp4r1by5t0_algo-1-2slct_1 ... \n",
      "\u001b[1BAttaching to tmp4r1by5t0_algo-1-2slct_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:19,097 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:19,106 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:19,120 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:19,124 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:19,490 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Generating setup.py\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:19,490 sagemaker-containers INFO     Generating setup.cfg\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:19,491 sagemaker-containers INFO     Generating MANIFEST.in\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:19,491 sagemaker-containers INFO     Installing module with the following command:\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m /opt/conda/bin/python -m pip install . -r requirements.txt\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Processing /tmp/tmptj8g8jqu/module_dir\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting numpy>=1.17.0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hCollecting transformers\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading transformers-4.1.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hCollecting datasets\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading datasets-1.1.3-py3-none-any.whl (153 kB)\n",
      "\u001b[K     |████████████████████████████████| 153 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hCollecting sklearn\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: sagemaker[local] in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.50.17)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting tokenizers==0.9.4\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (0.7)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (2.22.0)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (20.3)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting regex!=2019.12.17\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "\u001b[K     |████████████████████████████████| 723 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hCollecting sacremoses\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (4.42.1)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting filelock\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting xxhash\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hCollecting pyarrow>=0.17.1\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.7 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hCollecting multiprocess\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading multiprocess-0.70.11.1-py36-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 3.2 MB/s ta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hCollecting dill\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 3)) (0.25.0)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sklearn->-r requirements.txt (line 4)) (0.21.2)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (1.6.0)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: boto3>=1.10.44 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (1.13.1)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (1.2.2)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: smdebug-rulesconfig==0.1.2 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (0.1.2)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (0.1.5)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (3.11.3)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting docker-compose>=1.25.2; extra == \"local\"\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading docker_compose-1.27.4-py2.py3-none-any.whl (110 kB)\n",
      "\u001b[K     |████████████████████████████████| 110 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hRequirement already satisfied: PyYAML<6,>=5.3; extra == \"local\" in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (5.3.1)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; extra == \"local\" in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (1.25.8)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (2020.4.5.1)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.0.4)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (2.8)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 2)) (1.14.0)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 2)) (2.4.7)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 2)) (7.1.2)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 2)) (0.14.1)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.1)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2019.3)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker[local]->-r requirements.txt (line 5)) (3.1.0)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: botocore<1.17.0,>=1.16.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.10.44->sagemaker[local]->-r requirements.txt (line 5)) (1.16.1)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.10.44->sagemaker[local]->-r requirements.txt (line 5)) (0.9.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.10.44->sagemaker[local]->-r requirements.txt (line 5)) (0.3.3)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker[local]->-r requirements.txt (line 5)) (46.1.3.post20200330)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting distro<2,>=1.5.0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading distro-1.5.0-py2.py3-none-any.whl (18 kB)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting docker[ssh]<5,>=4.3.1\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading docker-4.4.0-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hCollecting texttable<2,>=0.9.0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading texttable-1.6.3-py2.py3-none-any.whl (10 kB)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting dockerpty<1,>=0.4.1\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading dockerpty-0.4.1.tar.gz (13 kB)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting cached-property<2,>=1.2.0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting jsonschema<4,>=2.5.1\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hCollecting websocket-client<1,>=0.32.0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hCollecting docopt<1,>=0.6.1\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting python-dotenv<1,>=0.13.0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading python_dotenv-0.15.0-py2.py3-none-any.whl (18 kB)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.1->boto3>=1.10.44->sagemaker[local]->-r requirements.txt (line 5)) (0.15.2)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: paramiko>=2.4.2; extra == \"ssh\" in /opt/conda/lib/python3.6/site-packages (from docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (2.7.1)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Collecting pyrsistent>=0.14.0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading pyrsistent-0.17.3.tar.gz (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hCollecting attrs>=17.4.0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25hRequirement already satisfied: pynacl>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (1.3.0)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: bcrypt>=3.1.3 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (3.1.7)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: cryptography>=2.5 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (2.9.2)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: cffi>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from pynacl>=1.0.1->paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (1.14.0)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.4.1->pynacl>=1.0.1->paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (2.20)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Building wheels for collected packages: sklearn, default-user-module-name, sacremoses, dockerpty, docopt, pyrsistent\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=8dd73b167efa352012f4d60a8da8c418f4ac6b01fdee38dd8f555c86affcfaf3\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Building wheel for default-user-module-name (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25h  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=6481 sha256=459fc8f5204b8120a7d86f9079bab423bf5e81a8177c798b3ffdb10801ebecf0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Stored in directory: /tmp/pip-ephem-wheel-cache-mw6iswy8/wheels/61/d8/d3/8752f402133d7b6280c1450d462fc039f41371b4efd5f6808c\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=1ab6ab86da92a1db27a986c15905d66885820dca09ab8fb21ba9899113aba16d\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Building wheel for dockerpty (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25h  Created wheel for dockerpty: filename=dockerpty-0.4.1-py3-none-any.whl size=16604 sha256=8eec83539783599a391eef7a73d653bbc5736edfb5bc51942fc2cde3301a4ebb\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/61/8f/e3/247046231ee138b48be905e4a748d570630e1f3ec24632b00b\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=17458787245c7aee10a5e5852b9ec6175d2b6e76d508c4dce66699c1c887af78\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/3f/2a/fa/4d7a888e69774d5e6e855d190a8a51b357d77cc05eb1c097c9\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Building wheel for pyrsistent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[?25h  Created wheel for pyrsistent: filename=pyrsistent-0.17.3-cp36-cp36m-linux_x86_64.whl size=112542 sha256=4f5e70bfb39164bf6b1bc05882932fd340d8c7cc39cba3b00f2378fe85e5143a\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/34/13/19/294da8e11bce7e563afee51251b9fa878185e14f4b5caf00cb\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Successfully built sklearn default-user-module-name sacremoses dockerpty docopt pyrsistent\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Installing collected packages: numpy, tokenizers, regex, sacremoses, filelock, transformers, xxhash, pyarrow, dill, multiprocess, datasets, sklearn, default-user-module-name, distro, websocket-client, docker, texttable, dockerpty, cached-property, pyrsistent, attrs, jsonschema, docopt, python-dotenv, docker-compose\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m   Attempting uninstall: numpy\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     Found existing installation: numpy 1.16.4\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     Uninstalling numpy-1.16.4:\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m       Successfully uninstalled numpy-1.16.4\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Successfully installed attrs-20.3.0 cached-property-1.5.2 datasets-1.1.3 default-user-module-name-1.0.0 dill-0.3.3 distro-1.5.0 docker-4.4.0 docker-compose-1.27.4 dockerpty-0.4.1 docopt-0.6.2 filelock-3.0.12 jsonschema-3.2.0 multiprocess-0.70.11.1 numpy-1.19.4 pyarrow-2.0.0 pyrsistent-0.17.3 python-dotenv-0.15.0 regex-2020.11.13 sacremoses-0.0.43 sklearn-0.0 texttable-1.6.3 tokenizers-0.9.4 transformers-4.1.1 websocket-client-0.57.0 xxhash-2.0.0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \u001b[33mWARNING: You are using pip version 20.1; however, version 20.3.3 is available.\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:57,256 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:57,290 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:57,318 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:39:57,339 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"channel_input_dirs\": {},\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"current_host\": \"algo-1-2slct\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m         \"algo-1-2slct\"\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m         \"train-batch-size\": 32\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"input_data_config\": {},\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"job_name\": \"huggingface-2020-12-22-10-39-15-532\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"master_hostname\": \"algo-1-2slct\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-eu-central-1-415465391280/huggingface-2020-12-22-10-39-15-532/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m         \"current_host\": \"algo-1-2slct\",\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m             \"algo-1-2slct\"\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_HOSTS=[\"algo-1-2slct\"]\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_HPS={\"epochs\":1,\"train-batch-size\":32}\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-2slct\",\"hosts\":[\"algo-1-2slct\"]}\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_INPUT_DATA_CONFIG={}\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_CHANNELS=[]\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_CURRENT_HOST=algo-1-2slct\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-eu-central-1-415465391280/huggingface-2020-12-22-10-39-15-532/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1-2slct\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-2slct\"],\"hyperparameters\":{\"epochs\":1,\"train-batch-size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-2020-12-22-10-39-15-532\",\"log_level\":20,\"master_hostname\":\"algo-1-2slct\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-415465391280/huggingface-2020-12-22-10-39-15-532/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-2slct\",\"hosts\":[\"algo-1-2slct\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"1\",\"--train-batch-size\",\"32\"]\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m SM_HP_TRAIN-BATCH-SIZE=32\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m /opt/conda/bin/python train.py --epochs 1 --train-batch-size 32\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m \n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown size, total: 207.28 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3...\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3. Subsequent calls will reuse this data.\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 2020-12-22 10:43:11,299 sagemaker-containers ERROR    ExecuteUserScriptError:\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Command \"/opt/conda/bin/python train.py --epochs 1 --train-batch-size 32\"\n",
      "Downloading: 4.67kB [00:00, 1.07MB/s]                   \n",
      "Downloading: 2.02kB [00:00, 675kB/s]                    \n",
      "Downloading: 100%|ââââââââââ| 84.1M/84.1M [00:46<00:00, 1.82MB/s]]\n",
      "12/22/2020 10:41:23 - INFO - filelock -   Lock 139798096869416 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\n",
      "Downloading: 100%|ââââââââââ| 442/442 [00:00<00:00, 208kB/s]\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 12/22/2020 10:41:24 - INFO - filelock -   Lock 139798096869416 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 12/22/2020 10:41:24 - INFO - filelock -   Lock 139798431762864 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\n",
      "Downloading: 100%|ââââââââââ| 268M/268M [01:23<00:00, 3.21MB/s]\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 12/22/2020 10:42:48 - INFO - filelock -   Lock 139798431762864 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 12/22/2020 10:42:51 - INFO - filelock -   Lock 139797347007904 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|ââââââââââ| 232k/232k [00:01<00:00, 129kB/s]\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 12/22/2020 10:42:53 - INFO - filelock -   Lock 139797347007904 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 12/22/2020 10:42:54 - INFO - filelock -   Lock 139797347004600 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|ââââââââââ| 466k/466k [00:02<00:00, 166kB/s]\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 12/22/2020 10:42:57 - INFO - filelock -   Lock 139797347004600 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "100%|ââââââââââ| 1/1 [00:00<00:00, 11.93ba/s]\n",
      "100%|ââââââââââ| 1/1 [00:00<00:00, 184.04ba/s]\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 12/22/2020 10:42:58 - INFO - __main__ -   Sample 38 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 2096, 3241, 1997, 1000, 1996, 2307, 4019, 1000, 1045, 3039, 2026, 2568, 2000, 17677, 2067, 2000, 2023, 2210, 17070, 1997, 1037, 3185, 2013, 2026, 5593, 1012, 1045, 2018, 3191, 1998, 2128, 1011, 3191, 1996, 18534, 3117, 2013, 4085, 2029, 4427, 2009, 1010, 1998, 2043, 2009, 2234, 2000, 1996, 2069, 5988, 1006, 2057, 2196, 2109, 2008, 2773, 2059, 1010, 2941, 1007, 1999, 2237, 2008, 3662, 1000, 3097, 1000, 3152, 1010, 1045, 2001, 2034, 1999, 2240, 2000, 4965, 2026, 7281, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2004, 2619, 2716, 2039, 2006, 12498, 2739, 9910, 4877, 1998, 10398, 3152, 2076, 25755, 1010, 1045, 2018, 2019, 18568, 3037, 1999, 11131, 1996, 22213, 1997, 2008, 4736, 2004, 7686, 1999, 1996, 13251, 1998, 3441, 1997, 2273, 2040, 2020, 2045, 1999, 2711, 1012, 2008, 3668, 2101, 2000, 1037, 10326, 19732, 2058, 1996, 2086, 2000, 4965, 2151, 2338, 2006, 1996, 3395, 1010, 1998, 2776, 2000, 3191, 1996, 8053, 17075, 6002, 1997, 7003, 24515, 11382, 12096, 1998, 17513, 3814, 2128, 7849, 4226, 1010, 2029, 3024, 2019, 2130, 6748, 12411, 28255, 1012, 1996, 3185, 4617, 1010, 2174, 1010, 2020, 4406, 2023, 2028, 1999, 2008, 2027, 6524, 5359, 1996, 5350, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5396, 1997, 2304, 1011, 1998, 1011, 2317, 2143, 2038, 2196, 2042, 2366, 2061, 2092, 2004, 2009, 2001, 1999, 2216, 2086, 1012, 1045, 2031, 2196, 2464, 2151, 6627, 8713, 12898, 2099, 2544, 1997, 2162, 2008, 3849, 2004, 14469, 2004, 2079, 1996, 2784, 9610, 10464, 28817, 7352, 1997, 3152, 2066, 1000, 1996, 4799, 3586, 1012, 1000, 2065, 2009, 2003, 2995, 2008, 2057, 2024, 16036, 2467, 2000, 2022, 12481, 2000, 1996, 4871, 1997, 2256, 5593, 1010, 2059, 1045, 18766, 2009, 10350, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1998, 2045, 2097, 2196, 2022, 2178, 1996, 7777, 1997, 6688, 8991, 2078, 2004, 1996, 16199, 12070, 2329, 2162, 5394, 2006, 2143, 1012, 2025, 2130, 2909, 9752, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'text': 'While thinking of \"The Great Escape\" I allowed my mind to wander back to this little gem of a movie from my childhood. I had read and re-read the autobiographical novel from 1949 which inspired it, and when it came to the only cinema (we never used that word then , actually) in town that showed \"foreign\" films, I was first in line to buy my ticket.<br /><br />As someone brought up on wartime newsreels and propaganda films during WWII, I had an avid interest in exploring the realities of that conflict as reflected in the memoirs and stories of men who were there in person. That extended later to a keen willingness over the years to buy any book on the subject, and eventually to read the equally compelling novels of Hans Helmut Kirst and Erich Maria Remarque, which provided an even deeper sensibility. The movie versions, however, were unlike this one in that they rarely delivered the goods.<br /><br />The medium of black-and-white film has never been served so well as it was in those years. I have never seen any technicolor version of war that seems as authentic as do the deep chiaroscuros of films like \"The Wooden Horse.\" If it is true that we are destined always to be captive to the images of our childhood, then I confess it freely.<br /><br />And there will never be another the likes of Leo Genn as the emblematic British war hero on film. Not even Sir Alec.'}.\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 12/22/2020 10:42:58 - INFO - __main__ -   Sample 74 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 7929, 1011, 2017, 2215, 2000, 3231, 8307, 2006, 2129, 6625, 2027, 2024, 2007, 2037, 29101, 1998, 1996, 16436, 1998, 29310, 9289, 3431, 1996, 6657, 1011, 2059, 2131, 2037, 6234, 4668, 2013, 3666, 2023, 2039, 3217, 16843, 2271, 9986, 2055, 4268, 2437, 14286, 7882, 5469, 17312, 2015, 1999, 1996, 9282, 3770, 1005, 1055, 1012, 2062, 2084, 2151, 3185, 1045, 2038, 2412, 2464, 1010, 1996, 2143, 9144, 2007, 10859, 2075, 13798, 1998, 13059, 1999, 1037, 2126, 2008, 2003, 3294, 2529, 1010, 2196, 10634, 1010, 1998, 12953, 1999, 1996, 2785, 1997, 16112, 15003, 1997, 3360, 2008, 2003, 12532, 20186, 2094, 2011, 1996, 3571, 1011, 10424, 2368, 14272, 2094, 4639, 2088, 2073, 2151, 21864, 8024, 1999, 3360, 2003, 11459, 2000, 2505, 2013, 19272, 2000, 2566, 27774, 1012, 7163, 1011, 9587, 24848, 12270, 2358, 9013, 3849, 2000, 2022, 4285, 2005, 1037, 6748, 4824, 1997, 2010, 10911, 2015, 1998, 28616, 23795, 2015, 2004, 1996, 12626, 1997, 9384, 4845, 1005, 1055, 2088, 1012, 2054, 2002, 4858, 1999, 2370, 1998, 2500, 3475, 1005, 1056, 2467, 3492, 1011, 2021, 3065, 2129, 2028, 2064, 5335, 1998, 21063, 2007, 2287, 1012, 2054, 2515, 2689, 2812, 2302, 9185, 1012, 1045, 2293, 2023, 3185, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'text': \"OK - you want to test somebody on how comfortable they are with their adolescence and the embarrassing and maniacal changes therin - then get their immediate reaction from watching this uproarious doc about kids making socially relevant horror flicks in the suburban 80's. More than any movie I has ever seen, the film deals with burdening sexuality and ego in a way that is completely human, never dull, and flushed in the kind of inherent goodness of youth that is discolored by the fear-frenzied adult world where any quirk in youth is accredited to anything from insanity to perversion. Mini-mogul Darren Stien seems to be reaching for a deeper understanding of his triumphs and misgivings as the patriarch of strict kid's world. What he finds in himself and others isn't always pretty - but shows how one can improve and reconcile with age. What does change mean without reflection. I love this movie.\"}.\n",
      "\u001b[36malgo-1-2slct_1  |\u001b[0m 12/22/2020 10:42:58 - INFO - __main__ -   Sample 30 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 10430, 2015, 4832, 2041, 2004, 2019, 2250, 26143, 1010, 8790, 8993, 1997, 2137, 2166, 1010, 1998, 2129, 1996, 2137, 3325, 2003, 5044, 1998, 4225, 2011, 2769, 1012, 2011, 4292, 1996, 2466, 1999, 1996, 23689, 17301, 1997, 1996, 13607, 1010, 2585, 5252, 11027, 2015, 2035, 13500, 2000, 1037, 20696, 1010, 2659, 2000, 1996, 2598, 1998, 13848, 2812, 21933, 23808, 6820, 7542, 1997, 1996, 2695, 1011, 2715, 3690, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2296, 2839, 5836, 1037, 2227, 2102, 1997, 2137, 3068, 1012, 4116, 10430, 4654, 6633, 24759, 14144, 1996, 19337, 5243, 9077, 5596, 2551, 10551, 1010, 7950, 2090, 6904, 4328, 6632, 2140, 4611, 1998, 1037, 2342, 2000, 2562, 2010, 1000, 2449, 1000, 2006, 2019, 2130, 19602, 1012, 1996, 19143, 1997, 2122, 2048, 2477, 2003, 1996, 23934, 2008, 7906, 1996, 2466, 3048, 2830, 1012, 1996, 3494, 1997, 5696, 1010, 2703, 2666, 1010, 1998, 6173, 8339, 1996, 8884, 1011, 2021, 2969, 1011, 3529, 2104, 11227, 2556, 1999, 2296, 6960, 1010, 2040, 2024, 9480, 2041, 1997, 13185, 2738, 2084, 7857, 1012, 2007, 1996, 2839, 1997, 6798, 1010, 3533, 6090, 3406, 15204, 2080, 8927, 1037, 8235, 7613, 1997, 1996, 23916, 18224, 15069, 1010, 1037, 6389, 6883, 2040, 1005, 1055, 24299, 2229, 1997, 4808, 2024, 13800, 3973, 2041, 27204, 9072, 2011, 2010, 14726, 7414, 2373, 1012, 1998, 11282, 9610, 7231, 3366, 2003, 1996, 7209, 6454, 1997, 1996, 3424, 16211, 3064, 2214, 1011, 3457, 1010, 2029, 9319, 2373, 2083, 2511, 6550, 1998, 1996, 2342, 1997, 1996, 2039, 1011, 1998, 1011, 2272, 2869, 2000, 13366, 2571, 6593, 7499, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2295, 11113, 8180, 3512, 1998, 5681, 14888, 1010, 1996, 10430, 2015, 2038, 3687, 2049, 2173, 2004, 1996, 7209, 2694, 3689, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 8827, 1037, 2204, 7452, 3538, 2000, 5252, 1005, 1055, 2186, 2052, 2022, 1996, 6099, 1010, 2178, 6355, 3689, 2008, 9020, 2000, 2191, 1996, 1057, 25394, 4355, 1997, 3494, 5875, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'text': 'The Sopranos stands out as an airtight, dynamic exploration of American life, and how the American experience is shaped and defined by money. By setting the story in the milieu of the underworld, David Chase eliminates all barriers to a grunt, low to the ground and outright mean deconstruction of the post-modern era. <br /><br />Every character represents a facet of American industry. Tony Soprano exemplifies the beleaguered working stiff, torn between familial duty and a need to keep his \"business\" on an even keel. The convergence of these two things is the imperative that keeps the story moving forward. The characters of Christopher, Paulie, and Bobby reflect the loyal - but self-serving underlings present in every enterprise, who are trusted out of necessity rather than merit. With the character of Ralph, Joe Pantoliano essays a brilliant interpretation of the charismatic psychopath, a twisted businessman who\\'s flourishes of violence are tragically outweighed by his stunning earning power. And Dominic Chianese is the ultimate symbol of the antiquated old-guard, which maintains power through established relationships and the need of the up- and-comers to deflect blame.<br /><br />Though abrasive and occasionally disturbing, The Sopranos has earned its place as the ultimate TV drama. <br /><br />PS A good companion piece to Chase\\'s series would be The Shield, another violent drama that manages to make the ugliest of characters interesting.'}.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/codebuild/output/src708968123/src/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mtmp4r1by5t0_algo-1-2slct_1 exited with code 1\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run: ['docker-compose', '-f', '/private/var/folders/jj/dzns9hc55db1vmfsjvrh9n8m0000gp/T/tmp4r1by5t0/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process exited with code: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Process exited with code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-df48364deffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpytorch_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \"\"\"\n\u001b[1;32m   1417\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         self.model_artifacts = self.container.train(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         )\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# which contains the exit code and append the command line to it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Failed to run: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcompose_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompose_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run: ['docker-compose', '-f', '/private/var/folders/jj/dzns9hc55db1vmfsjvrh9n8m0000gp/T/tmp4r1by5t0/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1"
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Estimator\n",
    "\n",
    "You run PyTorch training scripts on SageMaker by creating PyTorch Estimators. SageMaker training of your script is invoked when you call fit on a PyTorch Estimator. The following code sample shows how you train a custom PyTorch script `train.py`, passing in three hyperparameters (`epochs`). We are not going to pass any data into sagemaker training job instead it will be downloaded in `train.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "pytorch_estimator = PyTorch(entry_point='train.py',\n",
    "                            source_dir='src',\n",
    "                            sagemaker_session=sess,\n",
    "#                            use_spot_instances=True,\n",
    "#                            max_wait=7200, # Seconds to wait for spot instances to become available\n",
    "                            base_job_name='huggingface',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version='1.6.0',\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-22 11:08:27 Starting - Starting the training job...\n",
      "2020-12-22 11:08:50 Starting - Launching requested ML instancesProfilerReport-1608635306: InProgress\n",
      "......\n",
      "2020-12-22 11:09:51 Starting - Preparing the instances for training......\n",
      "2020-12-22 11:10:58 Downloading - Downloading input data\n",
      "2020-12-22 11:10:58 Training - Downloading the training image...\n",
      "2020-12-22 11:15:15 Uploading - Uploading generated training model\n",
      "2020-12-22 11:15:15 Completed - Training job completed\n",
      "ProfilerReport-1608635306: NoIssuesFound\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-12-22 11:12:44,121 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-12-22 11:12:44,144 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-12-22 11:12:47,170 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-22 11:12:47,459 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting transformers\n",
      "  Downloading transformers-4.1.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting datasets\n",
      "  Downloading datasets-1.1.3-py3-none-any.whl (153 kB)\u001b[0m\n",
      "\u001b[34mCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker[local] in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.72.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (20.4)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (4.46.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (0.7)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.9.4\n",
      "  Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (2.24.0)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\n",
      "  Downloading xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 3)) (1.1.3)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7 MB)\u001b[0m\n",
      "\u001b[34mCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.11.1-py36-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34mCollecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sklearn->-r requirements.txt (line 4)) (0.23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==0.1.4 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (0.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3>=1.14.12 in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (1.16.3)\u001b[0m\n",
      "\u001b[34mCollecting docker-compose>=1.25.2; extra == \"local\"\n",
      "  Downloading docker_compose-1.27.4-py2.py3-none-any.whl (110 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; extra == \"local\" in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML<6,>=5.3; extra == \"local\" in /opt/conda/lib/python3.6/site-packages (from sagemaker[local]->-r requirements.txt (line 5)) (5.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (2020.6.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (2.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 2)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 2)) (0.17.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2020.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 4)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker[local]->-r requirements.txt (line 5)) (50.3.0.post20201006)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker[local]->-r requirements.txt (line 5)) (3.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker[local]->-r requirements.txt (line 5)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker[local]->-r requirements.txt (line 5)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.20.0,>=1.19.3 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker[local]->-r requirements.txt (line 5)) (1.19.3)\u001b[0m\n",
      "\u001b[34mCollecting docker[ssh]<5,>=4.3.1\n",
      "  Downloading docker-4.4.0-py2.py3-none-any.whl (146 kB)\u001b[0m\n",
      "\u001b[34mCollecting websocket-client<1,>=0.32.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\u001b[0m\n",
      "\u001b[34mCollecting docopt<1,>=0.6.1\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-dotenv<1,>=0.13.0\n",
      "  Downloading python_dotenv-0.15.0-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting cached-property<2,>=1.2.0\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting texttable<2,>=0.9.0\n",
      "  Downloading texttable-1.6.3-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting distro<2,>=1.5.0\n",
      "  Downloading distro-1.5.0-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting dockerpty<1,>=0.4.1\n",
      "  Downloading dockerpty-0.4.1.tar.gz (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting jsonschema<4,>=2.5.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: paramiko>=2.4.2; extra == \"ssh\" in /opt/conda/lib/python3.6/site-packages (from docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (2.7.2)\u001b[0m\n",
      "\u001b[34mCollecting attrs>=17.4.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.17.3.tar.gz (106 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cryptography>=2.5 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (3.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pynacl>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: bcrypt>=3.1.3 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.5->paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (1.14.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.5->paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2; extra == \"local\"->sagemaker[local]->-r requirements.txt (line 5)) (2.20)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn, sacremoses, docopt, dockerpty, pyrsistent\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=0f399ba512a130dd5fbbc9f45baa0db6c095c9ed1e424e440efd24658537369c\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=20aea5d6267937edb941983f3371d729b3263840d2c533ecb226580e6aae6b57\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=cd88d2b7412c7e04d3231c12a06de8e2d1b0d4e164fe49d648f004b41b2ff6bd\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/2a/fa/4d7a888e69774d5e6e855d190a8a51b357d77cc05eb1c097c9\n",
      "  Building wheel for dockerpty (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for dockerpty (setup.py): finished with status 'done'\n",
      "  Created wheel for dockerpty: filename=dockerpty-0.4.1-py3-none-any.whl size=16605 sha256=ec5a8fcf1201e4b6a4ea56a170f9dee787368107ef68621a267945caa9834fbc\n",
      "  Stored in directory: /root/.cache/pip/wheels/61/8f/e3/247046231ee138b48be905e4a748d570630e1f3ec24632b00b\n",
      "  Building wheel for pyrsistent (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for pyrsistent (setup.py): finished with status 'done'\n",
      "  Created wheel for pyrsistent: filename=pyrsistent-0.17.3-cp36-cp36m-linux_x86_64.whl size=112545 sha256=313c71b69a2500127c350ebd237d207d821b61c10ed6018a7e3fd098f94c14b6\n",
      "  Stored in directory: /root/.cache/pip/wheels/34/13/19/294da8e11bce7e563afee51251b9fa878185e14f4b5caf00cb\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn sacremoses docopt dockerpty pyrsistent\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, filelock, tokenizers, sacremoses, transformers, xxhash, pyarrow, dill, multiprocess, datasets, sklearn, websocket-client, docker, docopt, python-dotenv, cached-property, texttable, distro, dockerpty, attrs, pyrsistent, jsonschema, docker-compose\u001b[0m\n",
      "\u001b[34mSuccessfully installed attrs-20.3.0 cached-property-1.5.2 datasets-1.1.3 dill-0.3.3 distro-1.5.0 docker-4.4.0 docker-compose-1.27.4 dockerpty-0.4.1 docopt-0.6.2 filelock-3.0.12 jsonschema-3.2.0 multiprocess-0.70.11.1 pyarrow-2.0.0 pyrsistent-0.17.3 python-dotenv-0.15.0 regex-2020.11.13 sacremoses-0.0.43 sklearn-0.0 texttable-1.6.3 tokenizers-0.9.4 transformers-4.1.1 websocket-client-0.57.0 xxhash-2.0.0\u001b[0m\n",
      "\u001b[34m2020-12-22 11:13:00,775 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 32,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-2020-12-22-11-08-26-639\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-22-11-08-26-639/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-22-11-08-26-639/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-2020-12-22-11-08-26-639\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-22-11-08-26-639/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --epochs 1 --train_batch_size 32\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown size, total: 207.28 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3...\u001b[0m\n",
      "\u001b[34mDataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mSample 450 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1045, 4067, 2643, 1045, 2134, 1005, 1056, 2175, 2000, 5988, 2005, 2023, 2143, 1012, 1045, 2052, 2022, 2200, 3374, 2005, 1996, 2769, 1045, 2435, 1012, 1045, 2387, 2009, 2006, 2694, 1998, 1045, 2481, 1005, 1056, 19337, 7416, 3726, 2026, 2159, 1012, 1045, 4687, 2065, 2151, 2143, 2071, 2022, 4788, 2084, 2023, 2028, 1012, 2027, 2985, 8817, 1997, 6363, 2000, 2023, 2143, 2005, 2498, 1012, 9643, 3772, 1998, 9643, 11967, 1012, 1045, 2228, 1996, 2060, 2111, 2040, 2626, 7928, 2024, 1996, 2158, 2551, 2013, 2008, 2143, 2194, 1025, 1007, 2009, 1005, 1055, 2200, 2502, 19807, 9363, 999, 1999, 2095, 2456, 2064, 2027, 2145, 4756, 2012, 2023, 2785, 1997, 2143, 1029, 7861, 20709, 18965, 1012, 1012, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'text': \"I thank god I didn't go to cinema for this film. I would be very sorry for the money I gave. I saw it on tv and I couldn't beleive my eyes. I wonder if any film could be worse than this one. they spent millions of dollars to this film for nothing. awful acting and awful scenario. I think the other people who wrote comments are the man working from that film company ;) it's very big fiasco! in year 2000 can they still laugh at this kind of film? embarassing...\"}.\u001b[0m\n",
      "\u001b[34mSample 299 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 2129, 1045, 2288, 2046, 2009, 1024, 2043, 1045, 2318, 3666, 2023, 2186, 2006, 9476, 2897, 1010, 1045, 2031, 2000, 2360, 2008, 1045, 1005, 2310, 2196, 2464, 2505, 2066, 2023, 1010, 1998, 2009, 2001, 1996, 2190, 1012, 2021, 2043, 1045, 2318, 9334, 1996, 2186, 2006, 17550, 1010, 1998, 2086, 2101, 2006, 4966, 2112, 1997, 24112, 2072, 1005, 1055, 8750, 9489, 6407, 1012, 2009, 2001, 6429, 1010, 1998, 5621, 4276, 3666, 1012, 2009, 2018, 1037, 2843, 1997, 20728, 2895, 2008, 2097, 6271, 2017, 2041, 1997, 2115, 2835, 1012, 1998, 1997, 2607, 1010, 1996, 4323, 2774, 1000, 2074, 4807, 1000, 1010, 1998, 6348, 6699, 1000, 2020, 1996, 2190, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 3494, 1010, 1998, 26152, 2015, 1024, 2026, 5440, 3494, 1999, 1996, 2265, 2020, 1024, 18235, 3217, 1010, 6829, 1010, 2128, 20844, 1010, 29461, 4697, 1010, 3203, 6151, 1010, 2053, 2378, 1010, 1998, 27838, 18069, 1012, 2026, 5440, 26152, 2015, 1999, 1996, 2265, 2008, 1045, 4669, 1996, 2087, 2024, 1996, 3358, 5717, 1010, 1998, 4958, 14001, 1010, 1998, 1997, 2607, 1996, 12456, 4948, 1010, 1998, 6677, 5666, 10760, 1045, 1010, 1998, 2462, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 3574, 1997, 1996, 2265, 1024, 2054, 2023, 2186, 2036, 4136, 2149, 2008, 1999, 2613, 2166, 1010, 5233, 2024, 2200, 2524, 1998, 2057, 2064, 2823, 2663, 1010, 2030, 4558, 1012, 2021, 3521, 2064, 2036, 2022, 2524, 2000, 6855, 1010, 1998, 1045, 2079, 2903, 1996, 26152, 8221, 2024, 2725, 1996, 2157, 2518, 1010, 1998, 2024, 2667, 2000, 6855, 2088, 3521, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2021, 2174, 1010, 2023, 2265, 2003, 5621, 1996, 2190, 1997, 1996, 2190, 1012, 2061, 1999, 5494, 2000, 2023, 3319, 1010, 2044, 2017, 3422, 2023, 2265, 1010, 2156, 1996, 3185, 10866, 17569, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'text': 'How I got into it: When I started watching this series on Cartoon Network,I have to say that I\\'ve never seen anything like this,and it was the best. But when I started collecting the series on VHS,and years later on DVD part of Bandai\\'s Anime Legends collections. It was amazing,and truly worth watching. It had a lot of exploding action that will blow you out of your seat. And of course,the theme songs \"Just Communication\",and Rhythm Emotions\" were the best.<br /><br />Characters,and Gundams: My favorite characters in the show were:Heero,Duo,Relena,Treize,Lady Und,Noin,and Zechs. My favorite Gundams in the show that I liked the most are the Wing Zero,and Epyon,and of course the Altron,and Deathscythe I,and II.<br /><br />Meaning of the show: What this series also tells us that in real life,wars are very hard and we can sometimes win,or lose. But peace can also be hard to obtain,and I do believe the Gundam pilots are doing the right thing,and are trying to obtain world peace.<br /><br />But however,this show is truly the best of the best. So in closing to this review,after you watch this show,see the Movie Endless Waltz.'}.\u001b[0m\n",
      "\u001b[34mSample 990 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 2023, 3185, 2003, 2672, 2028, 1997, 1996, 2087, 11771, 5691, 1997, 2456, 2008, 1045, 2031, 2464, 999, 2926, 1996, 2189, 11896, 2000, 3443, 23873, 2043, 2111, 3402, 10436, 1012, 2036, 5919, 2107, 2004, 7761, 2375, 2024, 2025, 5845, 2007, 1996, 4072, 27994, 1012, 1996, 2466, 2993, 2038, 3471, 1024, 1996, 4895, 2071, 2196, 2202, 2373, 2058, 1996, 2088, 2144, 1996, 2142, 2163, 2894, 2052, 2025, 3499, 2009, 2021, 3741, 2107, 2004, 2859, 1010, 3607, 1010, 2900, 1010, 4385, 1012, 2052, 2025, 2593, 1012, 2023, 2052, 2036, 2377, 2114, 2619, 2667, 2000, 2202, 2058, 1996, 2088, 2004, 17388, 2063, 29267, 25457, 2050, 2515, 1012, 2023, 15537, 2033, 1997, 2508, 5416, 5691, 1010, 2069, 2008, 2216, 2031, 2062, 2895, 999, 8100, 1996, 3185, 2003, 2081, 2005, 8135, 1998, 2069, 2005, 8135, 1998, 2027, 2089, 5959, 2009, 1012, 2144, 1045, 3685, 4175, 2870, 1037, 3017, 1045, 2424, 1996, 2878, 2801, 11320, 14808, 13288, 1012, 2023, 14951, 7297, 3849, 2000, 2022, 1010, 2065, 3373, 2000, 2022, 2995, 1010, 20754, 2485, 2000, 2060, 17678, 5369, 9243, 2011, 8754, 2015, 2005, 1996, 2203, 1997, 1996, 2088, 1012, 2339, 3571, 2107, 1037, 6061, 2043, 2057, 2064, 2191, 2166, 2004, 2204, 2004, 2825, 2182, 2006, 3011, 2302, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'text': 'This movie is maybe one of the most boring movies of 2000 that I have seen! Especially the music fails to create suspense when people suddenly disappear. Also aspects such as martial law are not treated with the necessary seriousness. The story itself has problems: the UN could never take power over the world since the United States alone would not allow it but nations such as China, Russia, Japan, etc. would not either. This would also play against someone trying to take over the world as Nicolae Carpathia does. This reminds me of James Bond movies, only that those have more action! Naturally the movie is made for Christians and only for Christians and they may enjoy it. Since I cannot count myself a Christian I find the whole idea ludicrous. This prophecy furthermore seems to be, if believed to be true, dangerously close to other prophecies by cults for the end of the world. Why fear such a possibility when we can make life as good as possible here on Earth without'}.\u001b[0m\n",
      "\u001b[34m[2020-12-22 11:14:03.900 algo-1:67 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-12-22 11:14:03.901 algo-1:67 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-12-22 11:14:03.901 algo-1:67 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-12-22 11:14:03.901 algo-1:67 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-12-22 11:14:03.923 algo-1:67 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-12-22 11:14:03.924 algo-1:67 INFO hook.py:459] Hook is writing from the hook with pid: 67\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-12-22 11:14:04.830 algo-1:67 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2020-12-22 11:14:04.830 algo-1:67 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2020-12-22 11:14:04.920 algo-1:67 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6814125776290894, 'eval_accuracy': 0.6, 'eval_f1': 0.23076923076923075, 'eval_precision': 0.75, 'eval_recall': 0.13636363636363635, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m2020-12-22 11:14:24,075 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 257\n",
      "Billable seconds: 257\n"
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
