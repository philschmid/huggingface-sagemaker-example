{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker example using `Trainer` class\n",
    "\n",
    "Each folder starting with `0X_..` contains an specific sagemaker example. Each example contains a jupyter notebooke `sagemaker-example.ipynb` and a `src/` folder. The `sagemaker-example` is a jupyter notebook which is used to train transformers i ncombination with datasets on AWS Sagemaker. The `src/` folder contains the `train.py`, our training script and `requirements.txt` for additional dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Sagemaker Session with local AWS Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From outside these notebooks, `get_execution_role()` will return an exception because it does not know what is the role name that SageMaker requires.\n",
    "\n",
    "To solve this issue, pass the IAM role name instead of using `get_execution_role()`.\n",
    "\n",
    "Therefore you have to create an IAM-Role with correct permission for sagemaker to start training jobs and download files from s3. Beware that you need s3 permission on bucket-level `\"arn:aws:s3:::sagemaker-*\"` and on object-level     `\"arn:aws:s3:::sagemaker-*/*\"`. \n",
    "\n",
    "You can read [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) how to create a role with right permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local aws profile configured in ~/.aws/credentials\n",
    "local_profile_name='hf-sm' # optional if you only have default configured\n",
    "\n",
    "# role name for sagemaker -> needs the described permissions from above\n",
    "role_name = \"SageMakerRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philipp to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::558105141721:role/SageMakerRole\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "try:\n",
    "    sess = sagemaker.Session()\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    import boto3\n",
    "    # creates a boto3 session using the local profile we defined\n",
    "    if local_profile_name:\n",
    "        os.environ['AWS_PROFILE'] = local_profile_name # setting env var bc local-mode cannot use boto3 session\n",
    "        #bt3 = boto3.session.Session(profile_name=local_profile_name)\n",
    "        #iam = bt3.client('iam')\n",
    "        # create sagemaker session with boto3 session\n",
    "        #sess = sagemaker.Session(boto_session=bt3)\n",
    "    iam = boto3.client('iam')\n",
    "    sess = sagemaker.Session()\n",
    "    # get role arn\n",
    "    role = iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    \n",
    "\n",
    "\n",
    "print(role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker Session prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/imdb/small/test/test_dataset.pt', 'datasets/imdb/small/training/train_dataset.pt']\n",
      "sagemaker-eu-central-1-558105141721\n",
      "eu-central-1\n"
     ]
    }
   ],
   "source": [
    "print(sess.list_s3_files(sess.default_bucket(),'datasets/')) # list objects in s3 under datsets/\n",
    "print(sess.default_bucket()) # s3 bucketname\n",
    "print(sess.boto_region_name) # aws region of sagemaker session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "Reusing dataset imdb (/Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "Loading cached shuffled indices for dataset at /Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-b6e0a86d50ba8f21.arrow\n",
      "Loading cached processed dataset at /Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-5cc5d886cb3db2a3.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0eb28449c27444ebae53578c8c39a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#helper tokenizer function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "# sample a to small dataset for training\n",
    "train_dataset = train_dataset.shuffle().select(range(2000)) # smaller the size for test dataset to 10k \n",
    "test_dataset = test_dataset.shuffle().select(range(150)) # smaller the size for test dataset to 10k \n",
    "\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset.rename_column_(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.rename_column_(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "# cach the dataset, so we can load it directly for training\n",
    "train_dataset.save_to_disk(\".\")\n",
    "torch.save(train_dataset, 'train_dataset.pt')\n",
    "torch.save(test_dataset, 'test_dataset.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to sagemaker S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def upload_data_to_s3(dataset=None,prefix='datasets',split_type='train'):\n",
    "    \"\"\"helper function with saves the dataset locally using dataset.save_to_disk() and upload its then to s3. \"\"\"\n",
    "    \n",
    "    temp_prefix =f\"{prefix}/{split_type}\"\n",
    "    # saves datasets in local directory\n",
    "    dataset.save_to_disk(f\"./{temp_prefix}\")\n",
    "    \n",
    "    # loops over saved files and uploads them to s3 \n",
    "    for file in glob.glob(f\"./{temp_prefix}/*\"):\n",
    "        sess.upload_data(file, key_prefix=temp_prefix)\n",
    "\n",
    "    # return s3 url to files for estimator.fit()\n",
    "    return f\"s3://{sess.default_bucket()}/{temp_prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-central-1-558105141721/datasets/imdb/small/train\n",
      "s3://sagemaker-eu-central-1-558105141721/datasets/imdb/small/test\n"
     ]
    }
   ],
   "source": [
    "prefix = 'datasets/imdb/small'\n",
    "\n",
    "training_input_path  = upload_data_to_s3(dataset=train_dataset,prefix=prefix,split_type='train')\n",
    "test_input_path      = upload_data_to_s3(dataset=test_dataset,prefix=prefix,split_type='test')\n",
    "\n",
    "print(training_input_path)\n",
    "print(test_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an local estimator for testing\n",
    "\n",
    "You run PyTorch training scripts on SageMaker by creating PyTorch Estimators. SageMaker training of your script is invoked when you call fit on a PyTorch Estimator. The following code sample shows how you train a custom PyTorch script `train.py`, passing in three hyperparameters (`epochs`). We are not going to pass any data into sagemaker training job instead it will be downloaded in `train.py`\n",
    "\n",
    "in sagemaker you can test you training in a \"local-mode\" by setting your instance_type to `'local'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSequenceClassification, Trainer, TrainingArguments\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score, precision_recall_fscore_support\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m ==\u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--eval-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(args.training_dir)\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(glob.glob(args.training_dir+\u001b[33m\"\u001b[39;49;00m\u001b[33m/*\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "    \u001b[37m# Get datasets\u001b[39;49;00m\r\n",
      "    train_dataset  = torch.load(os.path.join(args.training_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_dataset.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    test_dataset  = torch.load(os.path.join(args.test_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtest_dataset.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(train_dataset))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(test_dataset))\r\n",
      "\r\n",
      "    \u001b[37m# compute metrics function for binary classification\u001b[39;49;00m\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(pred):\r\n",
      "        labels = pred.label_ids\r\n",
      "        preds = pred.predictions.argmax(-\u001b[34m1\u001b[39;49;00m)\r\n",
      "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\u001b[33m'\u001b[39;49;00m\u001b[33mbinary\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        acc = accuracy_score(labels, preds)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: acc,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mf1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: f1,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mprecision\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: precision,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mrecall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: recall\r\n",
      "        }\r\n",
      "\r\n",
      "    \u001b[37m# define training args \u001b[39;49;00m\r\n",
      "    training_args = TrainingArguments(\r\n",
      "        output_dir=args.model_dir,\r\n",
      "        num_train_epochs=args.epochs,\r\n",
      "        per_device_train_batch_size=args.train_batch_size,\r\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\r\n",
      "        warmup_steps=args.warmup_steps,\r\n",
      "        evaluation_strategy=\u001b[33m'\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\r\n",
      "    trainer = Trainer(\r\n",
      "        model=model,\r\n",
      "        args=training_args,\r\n",
      "        compute_metrics=compute_metrics,\r\n",
      "        train_dataset=train_dataset,\r\n",
      "        eval_dataset=test_dataset\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[37m# train model\u001b[39;49;00m\r\n",
      "    trainer.train()\r\n",
      "\r\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\r\n",
      "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\r\n",
      "\r\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\r\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "     \u001b[37m# Saves the model to s3\u001b[39;49;00m\r\n",
      "    trainer.save_model(args.model_dir) \r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "pytorch_estimator = PyTorch(entry_point='train.py',\n",
    "                            source_dir='src',\n",
    "                            base_job_name='huggingface',\n",
    "                            instance_type='local',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version='1.5.0',\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmp7zytaq8k_algo-1-nck71_1 ... \n",
      "\u001b[1BAttaching to tmp7zytaq8k_algo-1-nck71_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:16,987 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:16,997 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:17,012 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:17,019 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:17,526 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Generating setup.py\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:17,527 sagemaker-containers INFO     Generating setup.cfg\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:17,527 sagemaker-containers INFO     Generating MANIFEST.in\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:17,527 sagemaker-containers INFO     Installing module with the following command:\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m /opt/conda/bin/python -m pip install . -r requirements.txt\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Processing /tmp/tmpf4laxu4y/module_dir\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Collecting numpy>=1.17.0\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25hCollecting transformers\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading transformers-4.1.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25hCollecting datasets\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading datasets-1.1.3-py3-none-any.whl (153 kB)\n",
      "\u001b[K     |████████████████████████████████| 153 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25hCollecting sklearn\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (20.3)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (0.7)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (2.22.0)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (4.42.1)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Collecting filelock\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Collecting regex!=2019.12.17\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "\u001b[K     |████████████████████████████████| 723 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25hCollecting sacremoses\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25hCollecting tokenizers==0.9.4\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25hCollecting multiprocess\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading multiprocess-0.70.11.1-py36-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 3.1 MB/s ta 0:00:01\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25hCollecting pyarrow>=0.17.1\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.7 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 3)) (0.25.0)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Collecting xxhash\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25hCollecting dill\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sklearn->-r requirements.txt (line 4)) (0.21.2)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 2)) (2.4.7)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 2)) (1.14.0)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (2.8)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.0.4)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (2020.4.5.1)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (1.25.8)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 2)) (7.1.2)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 2)) (0.14.1)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2019.3)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.1)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Requirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 4)) (1.2.2)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Building wheels for collected packages: sklearn, default-user-module-name, sacremoses\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=7ef7ae2f38586703c7e2dbdc68b8b80743118b41a3668fe637b4db9388288d55\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Building wheel for default-user-module-name (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25h  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=5640 sha256=95544b5f84f4e5bdab656db1c5f22da8c90db38f1438a6043783fc1cbf9f8035\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Stored in directory: /tmp/pip-ephem-wheel-cache-zo3fdgdv/wheels/d3/7f/34/4ea95dcb71dc8ee90188256947e0be123e36ffb0c6a74fee0b\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=cc3160b324a6e6c8199de9659ddf35f5794a7cfe29f8d4d81b3cccfe792c7e45\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Successfully built sklearn default-user-module-name sacremoses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Installing collected packages: numpy, filelock, regex, sacremoses, tokenizers, transformers, dill, multiprocess, pyarrow, xxhash, datasets, sklearn, default-user-module-name\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m   Attempting uninstall: numpy\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     Found existing installation: numpy 1.16.4\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     Uninstalling numpy-1.16.4:\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m       Successfully uninstalled numpy-1.16.4\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Successfully installed datasets-1.1.3 default-user-module-name-1.0.0 dill-0.3.3 filelock-3.0.12 multiprocess-0.70.11.1 numpy-1.19.4 pyarrow-2.0.0 regex-2020.11.13 sacremoses-0.0.43 sklearn-0.0 tokenizers-0.9.4 transformers-4.1.1 xxhash-2.0.0\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \u001b[33mWARNING: You are using pip version 20.1; however, version 20.3.3 is available.\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:44,873 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:44,899 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:44,919 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:19:44,933 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"current_host\": \"algo-1-nck71\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         \"algo-1-nck71\"\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         \"train_batch_size\": 32,\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         \"model_name\": \"distilbert-base-uncased\"\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         \"test\": {\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"job_name\": \"huggingface-2020-12-22-18-19-00-738\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"master_hostname\": \"algo-1-nck71\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-22-18-19-00-738/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         \"current_host\": \"algo-1-nck71\",\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m             \"algo-1-nck71\"\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_HOSTS=[\"algo-1-nck71\"]\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-nck71\",\"hosts\":[\"algo-1-nck71\"]}\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_CURRENT_HOST=algo-1-nck71\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-22-18-19-00-738/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-nck71\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-nck71\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-2020-12-22-18-19-00-738\",\"log_level\":20,\"master_hostname\":\"algo-1-nck71\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-22-18-19-00-738/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-nck71\",\"hosts\":[\"algo-1-nck71\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_HP_TRAIN_BATCH_SIZE=32\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m SM_HP_MODEL_NAME=distilbert-base-uncased\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m /opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m ['/opt/ml/input/data/train/dataset.arrow', '/opt/ml/input/data/train/dataset_info.json', '/opt/ml/input/data/train/ing', '/opt/ml/input/data/train/state.json']\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m ['/opt/ml/input/data/test/dataset.arrow', '/opt/ml/input/data/test/dataset_info.json', '/opt/ml/input/data/test/state.json', '/opt/ml/input/data/test/test_dataset.pt']\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2000\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-nck71_1  |\u001b[0m 2020-12-22 18:21:21,132 sagemaker-containers ERROR    ExecuteUserScriptError:\r\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Command \"/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\"\r\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \r",
      "Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]\r",
      "Downloading: 100%|ââââââââââ| 442/442 [00:00<00:00, 240kB/s]\r\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \r",
      "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]\r",
      "Downloading:   0%|          | 50.2k/268M [00:00<09:15, 482kB/s]\r",
      "Downloading:   0%|          | 295k/268M [00:00<07:02, 634kB/s] \r",
      "Downloading:   0%|          | 639k/268M [00:00<05:40, 785kB/s]\r",
      "Downloading:   0%|          | 1.31M/268M [00:00<04:09, 1.07MB/s]\r",
      "Downloading:   1%|          | 1.59M/268M [00:00<03:35, 1.24MB/s]\r",
      "Downloading:   1%|          | 2.11M/268M [00:00<02:46, 1.59MB/s]\r",
      "Downloading:   1%|          | 2.47M/268M [00:00<02:19, 1.90MB/s]\r",
      "Downloading:   1%|          | 2.81M/268M [00:00<02:01, 2.19MB/s]\r",
      "Downloading:   1%|          | 3.17M/268M [00:01<01:47, 2.46MB/s]\r",
      "Downloading:   1%|â         | 3.51M/268M [00:01<01:39, 2.66MB/s]\r",
      "Downloading:   1%|â         | 3.86M/268M [00:01<01:32, 2.86MB/s]\r",
      "Downloading:   2%|â         | 4.20M/268M [00:01<01:37, 2.70MB/s]\r",
      "Downloading:   2%|â         | 4.67M/268M [00:01<01:25, 3.10MB/s]\r",
      "Downloading:   2%|â         | 5.02M/268M [00:01<01:47, 2.45MB/s]\r",
      "Downloading:   2%|â         | 5.58M/268M [00:01<01:34, 2.77MB/s]\r",
      "Downloading:   2%|â         | 6.00M/268M [00:01<01:29, 2.94MB/s]\r",
      "Downloading:   2%|â         | 6.67M/268M [00:02<01:13, 3.54MB/s]\r",
      "Downloading:   3%|â         | 7.11M/268M [00:02<01:13, 3.53MB/s]\r",
      "Downloading:   3%|â         | 7.52M/268M [00:02<01:29, 2.90MB/s]\r",
      "Downloading:   3%|â         | 8.01M/268M [00:02<01:28, 2.93MB/s]\r",
      "Downloading:   3%|â         | 8.35M/268M [00:02<01:41, 2.56MB/s]\r",
      "Downloading:   3%|â         | 9.01M/268M [00:02<01:24, 3.05MB/s]\r",
      "Downloading:   3%|â         | 9.38M/268M [00:02<01:20, 3.20MB/s]\r",
      "Downloading:   4%|â         | 9.74M/268M [00:03<01:21, 3.18MB/s]\r",
      "Downloading:   4%|â         | 10.3M/268M [00:03<01:18, 3.26MB/s]\r",
      "Downloading:   4%|â         | 10.7M/268M [00:03<01:15, 3.42MB/s]\r",
      "Downloading:   4%|â         | 11.0M/268M [00:03<01:16, 3.34MB/s]\r",
      "Downloading:   4%|â         | 11.4M/268M [00:03<01:17, 3.31MB/s]\r",
      "Downloading:   5%|â         | 12.1M/268M [00:03<01:05, 3.90MB/s]\r",
      "Downloading:   5%|â         | 12.5M/268M [00:03<01:09, 3.66MB/s]\r",
      "Downloading:   5%|â         | 12.9M/268M [00:03<01:09, 3.70MB/s]\r",
      "Downloading:   5%|â         | 13.3M/268M [00:04<01:26, 2.95MB/s]\r",
      "Downloading:   5%|â         | 13.9M/268M [00:04<01:12, 3.48MB/s]\r",
      "Downloading:   5%|â         | 14.3M/268M [00:04<01:24, 3.01MB/s]\r",
      "Downloading:   5%|â         | 14.7M/268M [00:04<01:26, 2.93MB/s]\r",
      "Downloading:   6%|â         | 15.3M/268M [00:04<01:12, 3.49MB/s]\r",
      "Downloading:   6%|â         | 15.8M/268M [00:04<01:26, 2.92MB/s]\r",
      "Downloading:   6%|â         | 16.4M/268M [00:04<01:13, 3.44MB/s]\r",
      "Downloading:   6%|â         | 16.8M/268M [00:05<01:12, 3.47MB/s]\r",
      "Downloading:   6%|â         | 17.2M/268M [00:05<01:21, 3.09MB/s]\r",
      "Downloading:   7%|â         | 17.7M/268M [00:05<01:11, 3.49MB/s]\r",
      "Downloading:   7%|â         | 18.1M/268M [00:05<01:27, 2.87MB/s]\r",
      "Downloading:   7%|â         | 18.8M/268M [00:05<01:13, 3.41MB/s]\r",
      "Downloading:   7%|â         | 19.2M/268M [00:05<01:12, 3.43MB/s]\r",
      "Downloading:   7%|â         | 19.6M/268M [00:05<01:21, 3.07MB/s]\r",
      "Downloading:   7%|â         | 20.1M/268M [00:06<01:12, 3.42MB/s]\r",
      "Downloading:   8%|â         | 20.4M/268M [00:06<01:39, 2.48MB/s]\r",
      "Downloading:   8%|â         | 21.3M/268M [00:06<01:18, 3.15MB/s]\r",
      "Downloading:   8%|â         | 21.8M/268M [00:06<01:28, 2.79MB/s]\r",
      "Downloading:   8%|â         | 22.4M/268M [00:06<01:13, 3.33MB/s]\r",
      "Downloading:   9%|â         | 22.8M/268M [00:06<01:28, 2.78MB/s]\r",
      "Downloading:   9%|â         | 23.5M/268M [00:07<01:12, 3.37MB/s]\r",
      "Downloading:   9%|â         | 24.0M/268M [00:07<01:23, 2.93MB/s]\r",
      "Downloading:   9%|��         | 24.6M/268M [00:07<01:10, 3.45MB/s]\r",
      "Downloading:   9%|â         | 25.0M/268M [00:07<01:10, 3.45MB/s]\r",
      "Downloading:   9%|â         | 25.4M/268M [00:07<01:18, 3.09MB/s]\r",
      "Downloading:  10%|â         | 25.9M/268M [00:07<01:09, 3.47MB/s]\r",
      "Downloading:  10%|â         | 26.3M/268M [00:07<01:09, 3.47MB/s]\r",
      "Downloading:  10%|â         | 26.7M/268M [00:07<01:13, 3.28MB/s]\r",
      "Downloading:  10%|â         | 27.1M/268M [00:08<01:08, 3.51MB/s]\r",
      "Downloading:  10%|â         | 27.5M/268M [00:08<01:24, 2.86MB/s]\r",
      "Downloading:  10%|â         | 28.1M/268M [00:08<01:11, 3.38MB/s]\r",
      "Downloading:  11%|â         | 28.5M/268M [00:08<01:10, 3.39MB/s]\r",
      "Downloading:  11%|â         | 28.9M/268M [00:08<01:09, 3.42MB/s]\r",
      "Downloading:  11%|â         | 29.3M/268M [00:08<01:09, 3.41MB/s]\r",
      "Downloading:  11%|â         | 29.6M/268M [00:08<01:10, 3.40MB/s]\r",
      "Downloading:  11%|â         | 30.0M/268M [00:09<01:28, 2.68MB/s]\r",
      "Downloading:  11%|ââ        | 30.6M/268M [00:09<01:14, 3.17MB/s]\r",
      "Downloading:  12%|ââ        | 31.0M/268M [00:09<01:11, 3.30MB/s]\r",
      "Downloading:  12%|ââ        | 31.3M/268M [00:09<01:23, 2.84MB/s]\r",
      "Downloading:  12%|ââ        | 31.7M/268M [00:09<01:22, 2.86MB/s]\r",
      "Downloading:  12%|ââ        | 32.0M/268M [00:09<01:21, 2.90MB/s]\r",
      "Downloading:  12%|ââ        | 32.6M/268M [00:09<01:07, 3.47MB/s]\r",
      "Downloading:  12%|ââ        | 33.0M/268M [00:09<01:07, 3.47MB/s]\r",
      "Downloading:  12%|ââ        | 33.4M/268M [00:10<01:24, 2.78MB/s]\r",
      "Downloading:  13%|ââ        | 34.0M/268M [00:10<01:20, 2.90MB/s]\r",
      "Downloading:  13%|ââ        | 34.3M/268M [00:10<01:55, 2.03MB/s]\r",
      "Downloading:  13%|ââ        | 34.7M/268M [00:10<01:41, 2.30MB/s]\r",
      "Downloading:  13%|ââ        | 35.0M/268M [00:10<01:31, 2.55MB/s]\r",
      "Downloading:  13%|ââ        | 35.4M/268M [00:10<01:24, 2.75MB/s]\r",
      "Downloading:  13%|ââ        | 35.7M/268M [00:10<01:18, 2.96MB/s]\r",
      "Downloading:  13%|ââ        | 36.1M/268M [00:11<01:52, 2.06MB/s]\r",
      "Downloading:  14%|ââ        | 36.9M/268M [00:11<01:31, 2.53MB/s]\r",
      "Downloading:  14%|ââ        | 37.2M/268M [00:11<01:30, 2.55MB/s]\r",
      "Downloading:  14%|ââ        | 37.7M/268M [00:11<01:18, 2.94MB/s]\r",
      "Downloading:  14%|ââ        | 38.1M/268M [00:11<01:14, 3.09MB/s]\r",
      "Downloading:  14%|ââ        | 38.4M/268M [00:11<01:12, 3.18MB/s]\r",
      "Downloading:  14%|ââ        | 38.8M/268M [00:11<01:11, 3.21MB/s]\r",
      "Downloading:  15%|ââ        | 39.1M/268M [00:11<01:09, 3.28MB/s]\r",
      "Downloading:  15%|ââ        | 39.5M/268M [00:12<01:25, 2.67MB/s]\r",
      "Downloading:  15%|ââ        | 40.1M/268M [00:12<01:10, 3.21MB/s]\r",
      "Downloading:  15%|ââ        | 40.5M/268M [00:12<01:08, 3.34MB/s]\r",
      "Downloading:  15%|ââ        | 40.9M/268M [00:12<01:07, 3.35MB/s]\r",
      "Downloading:  15%|ââ        | 41.3M/268M [00:12<01:19, 2.86MB/s]\r",
      "Downloading:  16%|ââ        | 41.8M/268M [00:12<01:07, 3.34MB/s]\r",
      "Downloading:  16%|ââ        | 42.2M/268M [00:12<01:06, 3.39MB/s]\r",
      "Downloading:  16%|ââ        | 42.6M/268M [00:13<01:11, 3.14MB/s]\r",
      "Downloading:  16%|ââ        | 43.0M/268M [00:13<01:05, 3.42MB/s]\r",
      "Downloading:  16%|ââ        | 43.4M/268M [00:13<01:21, 2.75MB/s]\r",
      "Downloading:  16%|ââ        | 43.8M/268M [00:13<01:17, 2.88MB/s]\r",
      "Downloading:  17%|ââ        | 44.5M/268M [00:13<01:04, 3.47MB/s]\r",
      "Downloading:  17%|ââ        | 44.9M/268M [00:13<01:13, 3.02MB/s]\r",
      "Downloading:  17%|ââ        | 45.4M/268M [00:13<01:04, 3.44MB/s]\r",
      "Downloading:  17%|ââ        | 45.9M/268M [00:13<01:03, 3.53MB/s]\r",
      "Downloading:  17%|ââ        | 46.3M/268M [00:14<01:08, 3.25MB/s]\r",
      "Downloading:  17%|ââ        | 46.7M/268M [00:14<01:05, 3.40MB/s]\r",
      "Downloading:  18%|ââ        | 47.0M/268M [00:14<01:02, 3.54MB/s]\r",
      "Downloading:  18%|ââ        | 47.4M/268M [00:14<01:08, 3.22MB/s]\r",
      "Downloading:  18%|ââ        | 47.9M/268M [00:14<01:02, 3.54MB/s]\r",
      "Downloading:  18%|ââ        | 48.3M/268M [00:14<01:03, 3.48MB/s]\r",
      "Downloading:  18%|ââ        | 48.6M/268M [00:14<01:14, 2.93MB/s]\r",
      "Downloading:  18%|ââ        | 49.0M/268M [00:14<01:17, 2.81MB/s]\r",
      "Downloading:  18%|ââ        | 49.4M/268M [00:15<01:07, 3.23MB/s]\r",
      "Downloading:  19%|ââ        | 49.8M/268M [00:15<01:06, 3.28MB/s]\r",
      "Downloading:  19%|ââ        | 50.3M/268M [00:15<00:59, 3.66MB/s]\r",
      "Downloading:  19%|ââ        | 50.7M/268M [00:15<01:00, 3.59MB/s]\r",
      "Downloading:  19%|ââ        | 51.1M/268M [00:15<01:05, 3.32MB/s]\r",
      "Downloading:  19%|ââ        | 51.6M/268M [00:15<01:01, 3.52MB/s]\r",
      "Downloading:  19%|ââ        | 51.9M/268M [00:15<01:01, 3.54MB/s]\r",
      "Downloading:  20%|ââ        | 52.3M/268M [00:15<01:09, 3.12MB/s]\r",
      "Downloading:  20%|ââ        | 52.8M/268M [00:15<01:01, 3.48MB/s]\r",
      "Downloading:  20%|ââ        | 53.2M/268M [00:16<01:01, 3.50MB/s]\r",
      "Downloading:  20%|ââ        | 53.5M/268M [00:16<01:06, 3.23MB/s]\r",
      "Downloading:  20%|ââ        | 54.0M/268M [00:16<01:01, 3.49MB/s]\r",
      "Downloading:  20%|ââ        | 54.3M/268M [00:16<01:16, 2.78MB/s]\r",
      "Downloading:  21%|ââ        | 55.0M/268M [00:16<01:03, 3.33MB/s]\r",
      "Downloading:  21%|ââ        | 55.4M/268M [00:16<01:03, 3.36MB/s]\r",
      "Downloading:  21%|ââ        | 55.8M/268M [00:16<01:12, 2.93MB/s]\r",
      "Downloading:  21%|ââ        | 56.3M/268M [00:16<01:02, 3.41MB/s]\r",
      "Downloading:  21%|ââ        | 56.7M/268M [00:17<01:01, 3.41MB/s]\r",
      "Downloading:  21%|âââ       | 57.1M/268M [00:17<01:06, 3.15MB/s]\r",
      "Downloading:  21%|âââ       | 57.5M/268M [00:17<01:01, 3.43MB/s]\r",
      "Downloading:  22%|âââ       | 57.9M/268M [00:17<01:14, 2.81MB/s]\r",
      "Downloading:  22%|âââ       | 58.5M/268M [00:17<01:02, 3.35MB/s]\r",
      "Downloading:  22%|âââ       | 59.0M/268M [00:17<01:02, 3.32MB/s]\r",
      "Downloading:  22%|âââ       | 59.3M/268M [00:17<01:09, 2.99MB/s]\r",
      "Downloading:  22%|âââ       | 59.9M/268M [00:18<01:01, 3.37MB/s]\r",
      "Downloading:  22%|âââ       | 60.2M/268M [00:18<01:15, 2.75MB/s]\r",
      "Downloading:  23%|âââ       | 60.7M/268M [00:18<01:11, 2.90MB/s]\r",
      "Downloading:  23%|âââ       | 61.3M/268M [00:18<00:59, 3.47MB/s]\r",
      "Downloading:  23%|âââ       | 61.8M/268M [00:18<01:08, 2.99MB/s]\r",
      "Downloading:  23%|âââ       | 62.1M/268M [00:18<01:09, 2.95MB/s]\r",
      "Downloading:  23%|âââ       | 62.5M/268M [00:18<01:12, 2.82MB/s]\r",
      "Downloading:  23%|âââ       | 63.0M/268M [00:19<01:11, 2.88MB/s]\r",
      "Downloading:  24%|âââ       | 63.6M/268M [00:19<00:59, 3.43MB/s]\r",
      "Downloading:  24%|âââ       | 64.0M/268M [00:19<00:58, 3.46MB/s]\r",
      "Downloading:  24%|âââ       | 64.4M/268M [00:19<00:59, 3.43MB/s]\r",
      "Downloading:  24%|âââ       | 64.8M/268M [00:19<01:12, 2.81MB/s]\r",
      "Downloading:  24%|âââ       | 65.4M/268M [00:19<01:00, 3.35MB/s]\r",
      "Downloading:  25%|âââ       | 65.8M/268M [00:19<00:59, 3.40MB/s]\r",
      "Downloading:  25%|âââ       | 66.2M/268M [00:20<01:11, 2.83MB/s]\r",
      "Downloading:  25%|âââ       | 66.7M/268M [00:20<01:09, 2.89MB/s]\r",
      "Downloading:  25%|âââ       | 67.1M/268M [00:20<01:06, 3.04MB/s]\r",
      "Downloading:  25%|âââ       | 67.5M/268M [00:20<01:02, 3.20MB/s]\r",
      "Downloading:  25%|âââ       | 68.1M/268M [00:20<01:12, 2.77MB/s]\r",
      "Downloading:  26%|âââ       | 68.4M/268M [00:20<01:39, 2.01MB/s]\r",
      "Downloading:  26%|âââ       | 68.6M/268M [00:21<01:38, 2.03MB/s]\r",
      "Downloading:  26%|âââ       | 68.9M/268M [00:21<01:39, 2.00MB/s]\r",
      "Downloading:  26%|âââ       | 69.2M/268M [00:21<01:27, 2.27MB/s]\r",
      "Downloading:  26%|âââ       | 69.6M/268M [00:21<01:18, 2.53MB/s]\r",
      "Downloading:  26%|âââ       | 69.9M/268M [00:21<01:11, 2.76MB/s]\r",
      "Downloading:  26%|âââ       | 70.3M/268M [00:21<01:07, 2.95MB/s]\r",
      "Downloading:  26%|âââ       | 70.6M/268M [00:21<01:04, 3.07MB/s]\r",
      "Downloading:  26%|âââ       | 71.0M/268M [00:21<01:02, 3.14MB/s]\r",
      "Downloading:  27%|âââ       | 71.3M/268M [00:21<01:16, 2.57MB/s]\r",
      "Downloading:  27%|âââ       | 71.9M/268M [00:22<01:02, 3.12MB/s]\r",
      "Downloading:  27%|âââ       | 72.3M/268M [00:22<01:00, 3.22MB/s]\r",
      "Downloading:  27%|âââ       | 72.7M/268M [00:22<01:15, 2.59MB/s]\r",
      "Downloading:  27%|âââ       | 73.4M/268M [00:22<01:01, 3.19MB/s]\r",
      "Downloading:  28%|âââ       | 73.8M/268M [00:22<01:11, 2.70MB/s]\r",
      "Downloading:  28%|âââ       | 74.4M/268M [00:22<00:59, 3.25MB/s]\r",
      "Downloading:  28%|âââ       | 74.9M/268M [00:23<01:10, 2.74MB/s]\r",
      "Downloading:  28%|âââ       | 75.5M/268M [00:23<00:58, 3.28MB/s]\r",
      "Downloading:  28%|âââ       | 75.9M/268M [00:23<00:55, 3.43MB/s]\r",
      "Downloading:  28%|âââ       | 76.3M/268M [00:23<00:55, 3.47MB/s]\r",
      "Downloading:  29%|âââ       | 76.7M/268M [00:23<01:01, 3.11MB/s]\r",
      "Downloading:  29%|âââ       | 77.2M/268M [00:23<00:54, 3.48MB/s]\r",
      "Downloading:  29%|âââ       | 77.6M/268M [00:23<01:04, 2.95MB/s]\r",
      "Downloading:  29%|âââ       | 78.2M/268M [00:23<00:55, 3.44MB/s]\r",
      "Downloading:  29%|âââ       | 78.6M/268M [00:24<00:54, 3.47MB/s]\r",
      "Downloading:  29%|âââ       | 79.0M/268M [00:24<01:06, 2.85MB/s]\r",
      "Downloading:  30%|âââ       | 79.6M/268M [00:24<00:56, 3.36MB/s]\r",
      "Downloading:  30%|âââ       | 80.0M/268M [00:24<00:54, 3.44MB/s]\r",
      "Downloading:  30%|âââ       | 80.4M/268M [00:24<00:57, 3.26MB/s]\r",
      "Downloading:  30%|âââ       | 80.8M/268M [00:24<00:53, 3.47MB/s]\r",
      "Downloading:  30%|âââ       | 81.2M/268M [00:24<00:54, 3.45MB/s]\r",
      "Downloading:  30%|âââ       | 81.6M/268M [00:24<01:03, 2.92MB/s]\r",
      "Downloading:  31%|âââ       | 82.1M/268M [00:25<00:54, 3.39MB/s]\r",
      "Downloading:  31%|âââ       | 82.5M/268M [00:25<01:14, 2.50MB/s]\r",
      "Downloading:  31%|âââ       | 83.2M/268M [00:25<01:04, 2.85MB/s]\r",
      "Downloading:  31%|âââ       | 83.6M/268M [00:25<00:59, 3.09MB/s]\r",
      "Downloading:  31%|ââââ      | 84.2M/268M [00:25<00:50, 3.66MB/s]\r",
      "Downloading:  32%|ââââ      | 84.7M/268M [00:25<00:50, 3.59MB/s]\r",
      "Downloading:  32%|ââââ      | 85.1M/268M [00:25<01:01, 2.97MB/s]\r",
      "Downloading:  32%|ââââ      | 85.7M/268M [00:26<00:52, 3.50MB/s]\r",
      "Downloading:  32%|ââââ      | 86.1M/268M [00:26<01:01, 2.97MB/s]\r",
      "Downloading:  32%|ââââ      | 86.7M/268M [00:26<00:52, 3.46MB/s]\r",
      "Downloading:  33%|ââââ      | 87.1M/268M [00:26<00:51, 3.51MB/s]\r",
      "Downloading:  33%|ââââ      | 87.5M/268M [00:26<01:03, 2.85MB/s]\r",
      "Downloading:  33%|ââââ      | 88.1M/268M [00:26<00:52, 3.40MB/s]\r",
      "Downloading:  33%|ââââ      | 88.6M/268M [00:26<00:52, 3.44MB/s]\r",
      "Downloading:  33%|ââââ      | 89.0M/268M [00:27<00:59, 3.01MB/s]\r",
      "Downloading:  33%|ââââ      | 89.5M/268M [00:27<00:51, 3.44MB/s]\r",
      "Downloading:  34%|ââââ      | 89.9M/268M [00:27<00:51, 3.47MB/s]\r",
      "Downloading:  34%|ââââ      | 90.3M/268M [00:27<00:54, 3.24MB/s]\r",
      "Downloading:  34%|ââââ      | 90.7M/268M [00:27<00:51, 3.47MB/s]\r",
      "Downloading:  34%|ââââ      | 91.1M/268M [00:27<01:01, 2.88MB/s]\r",
      "Downloading:  34%|ââââ      | 91.5M/268M [00:27<00:54, 3.24MB/s]\r",
      "Downloading:  34%|ââââ      | 91.9M/268M [00:28<01:00, 2.90MB/s]\r",
      "Downloading:  34%|ââââ      | 92.3M/268M [00:28<00:57, 3.04MB/s]\r",
      "Downloading:  35%|ââââ      | 93.0M/268M [00:28<00:48, 3.61MB/s]\r",
      "Downloading:  35%|ââââ      | 93.4M/268M [00:28<00:59, 2.94MB/s]\r",
      "Downloading:  35%|ââââ      | 94.0M/268M [00:28<00:49, 3.49MB/s]\r",
      "Downloading:  35%|ââââ      | 94.4M/268M [00:28<00:49, 3.53MB/s]\r",
      "Downloading:  35%|ââââ      | 94.9M/268M [00:28<01:08, 2.54MB/s]\r",
      "Downloading:  36%|ââââ      | 95.7M/268M [00:29<00:59, 2.89MB/s]\r",
      "Downloading:  36%|ââââ      | 96.1M/268M [00:29<01:00, 2.82MB/s]\r",
      "Downloading:  36%|ââââ      | 96.9M/268M [00:29<00:49, 3.48MB/s]\r",
      "Downloading:  36%|ââââ      | 97.3M/268M [00:29<00:49, 3.43MB/s]\r",
      "Downloading:  36%|ââââ      | 97.7M/268M [00:29<00:51, 3.32MB/s]\r",
      "Downloading:  37%|ââââ      | 98.1M/268M [00:29<00:48, 3.49MB/s]\r",
      "Downloading:  37%|ââââ      | 98.6M/268M [00:29<00:56, 2.98MB/s]\r",
      "Downloading:  37%|ââââ      | 98.9M/268M [00:30<01:00, 2.78MB/s]\r",
      "Downloading:  37%|ââââ      | 99.4M/268M [00:30<00:53, 3.17MB/s]\r",
      "Downloading:  37%|ââââ      | 99.9M/268M [00:30<00:51, 3.28MB/s]\r",
      "Downloading:  38%|ââââ      | 101M/268M [00:30<00:43, 3.81MB/s] \r",
      "Downloading:  38%|ââââ      | 101M/268M [00:30<00:53, 3.12MB/s]\r",
      "Downloading:  38%|ââââ      | 102M/268M [00:30<00:45, 3.65MB/s]\r",
      "Downloading:  38%|ââââ      | 102M/268M [00:30<00:51, 3.24MB/s]\r",
      "Downloading:  38%|ââââ      | 102M/268M [00:31<01:18, 2.10MB/s]\r",
      "Downloading:  38%|ââââ      | 103M/268M [00:31<01:09, 2.39MB/s]\r",
      "Downloading:  38%|ââââ      | 103M/268M [00:31<01:02, 2.62MB/s]\r",
      "Downloading:  39%|ââââ      | 103M/268M [00:31<00:58, 2.83MB/s]\r",
      "Downloading:  39%|ââââ      | 104M/268M [00:31<01:06, 2.46MB/s]\r",
      "Downloading:  39%|ââââ      | 104M/268M [00:31<00:54, 2.99MB/s]\r",
      "Downloading:  39%|ââââ      | 105M/268M [00:32<01:02, 2.59MB/s]\r",
      "Downloading:  39%|ââââ      | 105M/268M [00:32<00:51, 3.14MB/s]\r",
      "Downloading:  40%|ââââ      | 106M/268M [00:32<01:02, 2.60MB/s]\r",
      "Downloading:  40%|ââââ      | 106M/268M [00:32<00:57, 2.81MB/s]\r",
      "Downloading:  40%|ââââ      | 107M/268M [00:32<00:54, 2.98MB/s]\r",
      "Downloading:  40%|ââââ      | 107M/268M [00:32<00:51, 3.13MB/s]\r",
      "Downloading:  40%|ââââ      | 108M/268M [00:32<00:49, 3.22MB/s]\r",
      "Downloading:  40%|ââââ      | 108M/268M [00:32<00:48, 3.27MB/s]\r",
      "Downloading:  40%|ââââ      | 109M/268M [00:33<00:46, 3.39MB/s]\r",
      "Downloading:  41%|ââââ      | 109M/268M [00:33<00:48, 3.28MB/s]\r",
      "Downloading:  41%|ââââ      | 109M/268M [00:33<00:47, 3.32MB/s]\r",
      "Downloading:  41%|ââââ      | 110M/268M [00:33<00:40, 3.92MB/s]\r",
      "Downloading:  41%|ââââ      | 111M/268M [00:33<00:50, 3.13MB/s]\r",
      "Downloading:  41%|âââââ     | 111M/268M [00:33<00:42, 3.66MB/s]\r",
      "Downloading:  42%|âââââ     | 112M/268M [00:33<00:50, 3.10MB/s]\r",
      "Downloading:  42%|âââââ     | 112M/268M [00:34<00:43, 3.57MB/s]\r",
      "Downloading:  42%|âââââ     | 113M/268M [00:34<00:48, 3.22MB/s]\r",
      "Downloading:  42%|âââââ     | 113M/268M [00:34<00:43, 3.55MB/s]\r",
      "Downloading:  42%|âââââ     | 113M/268M [00:34<00:43, 3.54MB/s]\r",
      "Downloading:  42%|âââââ     | 114M/268M [00:34<00:44, 3.50MB/s]\r",
      "Downloading:  43%|âââââ     | 114M/268M [00:34<00:59, 2.59MB/s]\r",
      "Downloading:  43%|âââââ     | 115M/268M [00:34<00:53, 2.88MB/s]\r",
      "Downloading:  43%|âââââ     | 115M/268M [00:35<00:48, 3.17MB/s]\r",
      "Downloading:  43%|âââââ     | 116M/268M [00:35<00:41, 3.68MB/s]\r",
      "Downloading:  43%|âââââ     | 116M/268M [00:35<00:41, 3.63MB/s]\r",
      "Downloading:  44%|âââââ     | 117M/268M [00:35<00:51, 2.93MB/s]\r",
      "Downloading:  44%|âââââ     | 117M/268M [00:35<00:43, 3.46MB/s]\r",
      "Downloading:  44%|âââââ     | 118M/268M [00:35<00:52, 2.87MB/s]\r",
      "Downloading:  44%|âââââ     | 118M/268M [00:36<00:50, 2.96MB/s]\r",
      "Downloading:  44%|âââââ     | 119M/268M [00:36<00:48, 3.07MB/s]\r",
      "Downloading:  44%|âââââ     | 119M/268M [00:36<00:44, 3.35MB/s]\r",
      "Downloading:  45%|âââââ     | 120M/268M [00:36<00:38, 3.83MB/s]\r",
      "Downloading:  45%|âââââ     | 120M/268M [00:36<00:51, 2.86MB/s]\r",
      "Downloading:  45%|âââââ     | 121M/268M [00:36<00:42, 3.49MB/s]\r",
      "Downloading:  45%|âââââ     | 121M/268M [00:36<00:49, 2.95MB/s]\r",
      "Downloading:  46%|âââââ     | 122M/268M [00:36<00:41, 3.50MB/s]\r",
      "Downloading:  46%|âââââ     | 123M/268M [00:37<00:49, 2.91MB/s]\r",
      "Downloading:  46%|âââââ     | 123M/268M [00:37<00:41, 3.47MB/s]\r",
      "Downloading:  46%|âââââ     | 124M/268M [00:37<00:50, 2.87MB/s]\r",
      "Downloading:  46%|âââââ     | 124M/268M [00:37<00:41, 3.45MB/s]\r",
      "Downloading:  47%|âââââ     | 125M/268M [00:37<00:41, 3.45MB/s]\r",
      "Downloading:  47%|âââââ     | 125M/268M [00:37<00:49, 2.87MB/s]\r",
      "Downloading:  47%|âââââ     | 126M/268M [00:38<00:42, 3.37MB/s]\r",
      "Downloading:  47%|âââââ     | 126M/268M [00:38<01:05, 2.16MB/s]\r",
      "Downloading:  47%|âââââ     | 127M/268M [00:38<00:52, 2.69MB/s]\r",
      "Downloading:  47%|âââââ     | 127M/268M [00:38<00:48, 2.87MB/s]\r",
      "Downloading:  48%|âââââ     | 128M/268M [00:38<00:46, 3.02MB/s]\r",
      "Downloading:  48%|âââââ     | 128M/268M [00:38<00:44, 3.14MB/s]\r",
      "Downloading:  48%|âââââ     | 128M/268M [00:38<00:44, 3.16MB/s]\r",
      "Downloading:  48%|âââââ     | 129M/268M [00:39<00:42, 3.28MB/s]\r",
      "Downloading:  48%|âââââ     | 129M/268M [00:39<00:41, 3.34MB/s]\r",
      "Downloading:  48%|âââââ     | 129M/268M [00:39<00:52, 2.66MB/s]\r",
      "Downloading:  48%|âââââ     | 130M/268M [00:39<00:46, 2.98MB/s]\r",
      "Downloading:  49%|âââââ     | 130M/268M [00:39<00:39, 3.48MB/s]\r",
      "Downloading:  49%|âââââ     | 131M/268M [00:39<00:47, 2.87MB/s]\r",
      "Downloading:  49%|âââââ     | 132M/268M [00:39<00:39, 3.41MB/s]\r",
      "Downloading:  49%|âââââ     | 132M/268M [00:40<00:39, 3.41MB/s]\r",
      "Downloading:  49%|âââââ     | 132M/268M [00:40<00:47, 2.88MB/s]\r",
      "Downloading:  50%|âââââ     | 133M/268M [00:40<00:39, 3.38MB/s]\r",
      "Downloading:  50%|âââââ     | 133M/268M [00:40<00:39, 3.43MB/s]\r",
      "Downloading:  50%|âââââ     | 134M/268M [00:40<00:45, 2.94MB/s]\r",
      "Downloading:  50%|âââââ     | 134M/268M [00:40<00:39, 3.42MB/s]\r",
      "Downloading:  50%|âââââ     | 135M/268M [00:40<00:38, 3.45MB/s]\r",
      "Downloading:  50%|âââââ     | 135M/268M [00:40<00:41, 3.22MB/s]\r",
      "Downloading:  51%|âââââ     | 136M/268M [00:41<00:38, 3.46MB/s]\r",
      "Downloading:  51%|âââââ     | 136M/268M [00:41<01:06, 1.98MB/s]\r",
      "Downloading:  51%|âââââ     | 136M/268M [00:41<00:57, 2.27MB/s]\r",
      "Downloading:  51%|âââââ     | 137M/268M [00:41<01:01, 2.15MB/s]\r",
      "Downloading:  51%|âââââ     | 137M/268M [00:41<00:49, 2.64MB/s]\r",
      "Downloading:  51%|ââââââ    | 138M/268M [00:41<00:45, 2.85MB/s]\r",
      "Downloading:  51%|ââââââ    | 138M/268M [00:42<00:43, 3.00MB/s]\r",
      "Downloading:  52%|ââââââ    | 138M/268M [00:42<00:41, 3.14MB/s]\r",
      "Downloading:  52%|ââââââ    | 139M/268M [00:42<00:49, 2.64MB/s]\r",
      "Downloading:  52%|ââââââ    | 139M/268M [00:42<00:40, 3.18MB/s]\r",
      "Downloading:  52%|ââââââ    | 140M/268M [00:42<00:39, 3.21MB/s]\r",
      "Downloading:  52%|ââââââ    | 140M/268M [00:42<00:45, 2.83MB/s]\r",
      "Downloading:  52%|ââââââ    | 141M/268M [00:42<00:38, 3.32MB/s]\r",
      "Downloading:  53%|ââââââ    | 141M/268M [00:43<00:46, 2.73MB/s]\r",
      "Downloading:  53%|ââââââ    | 142M/268M [00:43<00:38, 3.31MB/s]\r",
      "Downloading:  53%|ââââââ    | 142M/268M [00:43<00:37, 3.32MB/s]\r",
      "Downloading:  53%|ââââââ    | 142M/268M [00:43<00:42, 2.98MB/s]\r",
      "Downloading:  53%|ââââââ    | 143M/268M [00:43<00:36, 3.42MB/s]\r",
      "Downloading:  53%|ââââââ    | 143M/268M [00:43<00:43, 2.85MB/s]\r",
      "Downloading:  54%|ââââââ    | 144M/268M [00:43<00:36, 3.40MB/s]\r",
      "Downloading:  54%|ââââââ    | 144M/268M [00:43<00:36, 3.38MB/s]\r",
      "Downloading:  54%|ââââââ    | 145M/268M [00:44<00:40, 3.06MB/s]\r",
      "Downloading:  54%|ââââââ    | 145M/268M [00:44<00:35, 3.46MB/s]\r",
      "Downloading:  54%|ââââââ    | 146M/268M [00:44<00:35, 3.42MB/s]\r",
      "Downloading:  55%|ââââââ    | 146M/268M [00:44<00:35, 3.44MB/s]\r",
      "Downloading:  55%|ââââââ    | 146M/268M [00:44<00:35, 3.42MB/s]\r",
      "Downloading:  55%|ââââââ    | 147M/268M [00:44<00:35, 3.41MB/s]\r",
      "Downloading:  55%|ââââââ    | 147M/268M [00:44<00:43, 2.75MB/s]\r",
      "Downloading:  55%|ââââââ    | 148M/268M [00:44<00:36, 3.30MB/s]\r",
      "Downloading:  55%|ââââââ    | 148M/268M [00:45<00:35, 3.34MB/s]\r",
      "Downloading:  55%|ââââââ    | 149M/268M [00:45<00:38, 3.12MB/s]\r",
      "Downloading:  56%|ââââââ    | 149M/268M [00:45<00:35, 3.40MB/s]\r",
      "Downloading:  56%|ââââââ    | 149M/268M [00:45<00:34, 3.45MB/s]\r",
      "Downloading:  56%|ââââââ    | 150M/268M [00:45<00:41, 2.85MB/s]\r",
      "Downloading:  56%|ââââââ    | 150M/268M [00:45<00:34, 3.37MB/s]\r",
      "Downloading:  56%|ââââââ    | 151M/268M [00:45<00:42, 2.77MB/s]\r",
      "Downloading:  56%|ââââââ    | 151M/268M [00:45<00:34, 3.33MB/s]\r",
      "Downloading:  57%|ââââââ    | 152M/268M [00:46<00:34, 3.36MB/s]\r",
      "Downloading:  57%|ââââââ    | 152M/268M [00:46<00:36, 3.16MB/s]\r",
      "Downloading:  57%|ââââââ    | 153M/268M [00:46<00:33, 3.45MB/s]\r",
      "Downloading:  57%|ââââââ    | 153M/268M [00:46<00:33, 3.44MB/s]\r",
      "Downloading:  57%|ââââââ    | 153M/268M [00:46<00:34, 3.34MB/s]\r",
      "Downloading:  57%|ââââââ    | 154M/268M [00:46<00:33, 3.42MB/s]\r",
      "Downloading:  58%|ââââââ    | 154M/268M [00:46<00:33, 3.45MB/s]\r",
      "Downloading:  58%|ââââââ    | 154M/268M [00:46<00:39, 2.91MB/s]\r",
      "Downloading:  58%|ââââââ    | 155M/268M [00:47<00:33, 3.41MB/s]\r",
      "Downloading:  58%|ââââââ    | 155M/268M [00:47<00:39, 2.83MB/s]\r",
      "Downloading:  58%|ââââââ    | 156M/268M [00:47<00:33, 3.36MB/s]\r",
      "Downloading:  58%|ââââââ    | 156M/268M [00:47<00:40, 2.76MB/s]\r",
      "Downloading:  59%|ââââââ    | 157M/268M [00:47<00:34, 3.21MB/s]\r",
      "Downloading:  59%|ââââââ    | 157M/268M [00:47<00:31, 3.55MB/s]\r",
      "Downloading:  59%|ââââââ    | 158M/268M [00:47<00:37, 2.91MB/s]\r",
      "Downloading:  59%|ââââââ    | 159M/268M [00:48<00:31, 3.47MB/s]\r",
      "Downloading:  59%|ââââââ    | 159M/268M [00:48<00:31, 3.46MB/s]\r",
      "Downloading:  59%|ââââââ    | 159M/268M [00:48<00:35, 3.07MB/s]\r",
      "Downloading:  60%|ââââââ    | 160M/268M [00:48<00:31, 3.43MB/s]\r",
      "Downloading:  60%|ââââââ    | 160M/268M [00:48<00:38, 2.83MB/s]\r",
      "Downloading:  60%|ââââââ    | 161M/268M [00:48<00:31, 3.41MB/s]\r",
      "Downloading:  60%|ââââââ    | 161M/268M [00:48<00:38, 2.78MB/s]\r",
      "Downloading:  60%|ââââââ    | 162M/268M [00:49<00:31, 3.34MB/s]\r",
      "Downloading:  61%|ââââââ    | 162M/268M [00:49<00:38, 2.75MB/s]\r",
      "Downloading:  61%|ââââââ    | 163M/268M [00:49<00:35, 2.95MB/s]\r",
      "Downloading:  61%|ââââââ    | 163M/268M [00:49<00:43, 2.41MB/s]\r",
      "Downloading:  61%|ââââââ    | 164M/268M [00:49<00:35, 2.97MB/s]\r",
      "Downloading:  61%|âââââââ   | 164M/268M [00:49<00:37, 2.74MB/s]\r",
      "Downloading:  61%|âââââââ   | 165M/268M [00:50<00:33, 3.04MB/s]\r",
      "Downloading:  62%|âââââââ   | 165M/268M [00:50<00:32, 3.19MB/s]\r",
      "Downloading:  62%|âââââââ   | 166M/268M [00:50<00:27, 3.74MB/s]\r",
      "Downloading:  62%|âââââââ   | 166M/268M [00:50<00:32, 3.11MB/s]\r",
      "Downloading:  62%|âââââââ   | 167M/268M [00:50<00:27, 3.62MB/s]\r",
      "Downloading:  62%|âââââââ   | 167M/268M [00:50<00:28, 3.57MB/s]\r",
      "Downloading:  63%|âââââââ   | 168M/268M [00:50<00:33, 2.98MB/s]\r",
      "Downloading:  63%|âââââââ   | 168M/268M [00:50<00:28, 3.47MB/s]\r",
      "Downloading:  63%|âââââââ   | 169M/268M [00:51<00:34, 2.84MB/s]\r",
      "Downloading:  63%|âââââââ   | 169M/268M [00:51<00:29, 3.40MB/s]\r",
      "Downloading:  63%|âââââââ   | 170M/268M [00:51<00:28, 3.41MB/s]\r",
      "Downloading:  63%|âââââââ   | 170M/268M [00:51<00:34, 2.81MB/s]\r",
      "Downloading:  64%|âââââââ   | 170M/268M [00:51<00:49, 1.98MB/s]\r",
      "Downloading:  64%|âââââââ   | 171M/268M [00:52<00:42, 2.27MB/s]\r",
      "Downloading:  64%|âââââââ   | 171M/268M [00:52<00:38, 2.54MB/s]\r",
      "Downloading:  64%|âââââââ   | 172M/268M [00:52<00:50, 1.90MB/s]\r",
      "Downloading:  64%|âââââââ   | 172M/268M [00:52<00:55, 1.73MB/s]\r",
      "Downloading:  64%|âââââââ   | 172M/268M [00:52<00:46, 2.08MB/s]\r",
      "Downloading:  64%|âââââââ   | 173M/268M [00:52<00:40, 2.37MB/s]\r",
      "Downloading:  65%|âââââââ   | 173M/268M [00:52<00:36, 2.61MB/s]\r",
      "Downloading:  65%|âââââââ   | 173M/268M [00:52<00:33, 2.80MB/s]\r",
      "Downloading:  65%|âââââââ   | 174M/268M [00:53<00:32, 2.94MB/s]\r",
      "Downloading:  65%|âââââââ   | 174M/268M [00:53<00:30, 3.10MB/s]\r",
      "Downloading:  65%|âââââââ   | 174M/268M [00:53<00:37, 2.53MB/s]\r",
      "Downloading:  65%|âââââââ   | 175M/268M [00:53<00:29, 3.10MB/s]\r",
      "Downloading:  65%|âââââââ   | 175M/268M [00:53<00:29, 3.17MB/s]\r",
      "Downloading:  66%|âââââââ   | 176M/268M [00:53<00:32, 2.88MB/s]\r",
      "Downloading:  66%|âââââââ   | 176M/268M [00:53<00:27, 3.30MB/s]\r",
      "Downloading:  66%|âââââââ   | 177M/268M [00:54<00:33, 2.71MB/s]\r",
      "Downloading:  66%|âââââââ   | 177M/268M [00:54<00:33, 2.69MB/s]\r",
      "Downloading:  66%|âââââââ   | 178M/268M [00:54<00:27, 3.31MB/s]\r",
      "Downloading:  66%|âââââââ   | 178M/268M [00:54<00:31, 2.83MB/s]\r",
      "Downloading:  67%|âââââââ   | 179M/268M [00:54<00:26, 3.39MB/s]\r",
      "Downloading:  67%|âââââââ   | 179M/268M [00:54<00:31, 2.84MB/s]\r",
      "Downloading:  67%|âââââââ   | 180M/268M [00:54<00:26, 3.35MB/s]\r",
      "Downloading:  67%|âââââââ   | 180M/268M [00:55<00:25, 3.49MB/s]\r",
      "Downloading:  67%|âââââââ   | 181M/268M [00:55<00:30, 2.89MB/s]\r",
      "Downloading:  68%|âââââââ   | 181M/268M [00:55<00:29, 2.97MB/s]\r",
      "Downloading:  68%|âââââââ   | 182M/268M [00:55<00:27, 3.15MB/s]\r",
      "Downloading:  68%|âââââââ   | 182M/268M [00:55<00:23, 3.71MB/s]\r",
      "Downloading:  68%|âââââââ   | 183M/268M [00:55<00:23, 3.64MB/s]\r",
      "Downloading:  68%|âââââââ   | 183M/268M [00:55<00:27, 3.06MB/s]\r",
      "Downloading:  69%|âââââââ   | 184M/268M [00:56<00:23, 3.52MB/s]\r",
      "Downloading:  69%|âââââââ   | 184M/268M [00:56<00:23, 3.53MB/s]\r",
      "Downloading:  69%|âââââââ   | 184M/268M [00:56<00:24, 3.46MB/s]\r",
      "Downloading:  69%|âââââââ   | 185M/268M [00:56<00:24, 3.46MB/s]\r",
      "Downloading:  69%|âââââââ   | 185M/268M [00:56<00:29, 2.79MB/s]\r",
      "Downloading:  69%|âââââââ   | 186M/268M [00:56<00:24, 3.34MB/s]\r",
      "Downloading:  69%|âââââââ   | 186M/268M [00:56<00:29, 2.76MB/s]\r",
      "Downloading:  70%|âââââââ   | 187M/268M [00:56<00:24, 3.33MB/s]\r",
      "Downloading:  70%|âââââââ   | 187M/268M [00:57<00:23, 3.38MB/s]\r",
      "Downloading:  70%|âââââââ   | 188M/268M [00:57<00:28, 2.81MB/s]\r",
      "Downloading:  70%|âââââââ   | 188M/268M [00:57<00:23, 3.35MB/s]\r",
      "Downloading:  70%|âââââââ   | 189M/268M [00:57<00:28, 2.81MB/s]\r",
      "Downloading:  71%|âââââââ   | 189M/268M [00:57<00:23, 3.37MB/s]\r",
      "Downloading:  71%|âââââââ   | 190M/268M [00:57<00:27, 2.82MB/s]\r",
      "Downloading:  71%|âââââââ   | 190M/268M [00:58<00:22, 3.38MB/s]\r",
      "Downloading:  71%|âââââââ   | 191M/268M [00:58<00:27, 2.82MB/s]\r",
      "Downloading:  71%|ââââââââ  | 192M/268M [00:58<00:22, 3.40MB/s]\r",
      "Downloading:  72%|ââââââââ  | 192M/268M [00:58<00:22, 3.40MB/s]\r",
      "Downloading:  72%|ââââââââ  | 192M/268M [00:58<00:26, 2.85MB/s]\r",
      "Downloading:  72%|ââââââââ  | 193M/268M [00:58<00:22, 3.40MB/s]\r",
      "Downloading:  72%|ââââââââ  | 194M/268M [00:58<00:26, 2.85MB/s]\r",
      "Downloading:  72%|ââââââââ  | 194M/268M [00:59<00:21, 3.40MB/s]\r",
      "Downloading:  73%|ââââââââ  | 195M/268M [00:59<00:21, 3.41MB/s]\r",
      "Downloading:  73%|ââââââââ  | 195M/268M [00:59<00:23, 3.15MB/s]\r",
      "Downloading:  73%|ââââââââ  | 195M/268M [00:59<00:25, 2.80MB/s]\r",
      "Downloading:  73%|ââââââââ  | 196M/268M [00:59<00:22, 3.27MB/s]\r",
      "Downloading:  73%|ââââââââ  | 196M/268M [00:59<00:20, 3.57MB/s]\r",
      "Downloading:  73%|ââââââââ  | 197M/268M [00:59<00:18, 3.75MB/s]\r",
      "Downloading:  74%|ââââââââ  | 197M/268M [00:59<00:18, 3.78MB/s]\r",
      "Downloading:  74%|ââââââââ  | 198M/268M [01:00<00:19, 3.66MB/s]\r",
      "Downloading:  74%|ââââââââ  | 198M/268M [01:00<00:19, 3.60MB/s]\r",
      "Downloading:  74%|ââââââââ  | 198M/268M [01:00<00:19, 3.54MB/s]\r",
      "Downloading:  74%|ââââââââ  | 199M/268M [01:00<00:19, 3.53MB/s]\r",
      "Downloading:  74%|ââââââââ  | 199M/268M [01:00<00:19, 3.49MB/s]\r",
      "Downloading:  74%|ââââââââ  | 199M/268M [01:00<00:24, 2.81MB/s]\r",
      "Downloading:  75%|ââââââââ  | 200M/268M [01:00<00:20, 3.34MB/s]\r",
      "Downloading:  75%|ââââââââ  | 200M/268M [01:00<00:20, 3.36MB/s]\r",
      "Downloading:  75%|ââââââââ  | 201M/268M [01:01<00:21, 3.06MB/s]\r",
      "Downloading:  75%|ââââââââ  | 201M/268M [01:01<00:19, 3.43MB/s]\r",
      "Downloading:  75%|ââââââââ  | 202M/268M [01:01<00:27, 2.45MB/s]\r",
      "Downloading:  75%|ââââââââ  | 202M/268M [01:01<00:23, 2.83MB/s]\r",
      "Downloading:  76%|ââââââââ  | 203M/268M [01:01<00:21, 3.00MB/s]\r",
      "Downloading:  76%|ââââââââ  | 203M/268M [01:01<00:20, 3.13MB/s]\r",
      "Downloading:  76%|ââââââââ  | 203M/268M [01:01<00:20, 3.18MB/s]\r",
      "Downloading:  76%|ââââââââ  | 204M/268M [01:02<00:35, 1.84MB/s]\r",
      "Downloading:  76%|ââââââââ  | 204M/268M [01:02<00:30, 2.09MB/s]\r",
      "Downloading:  76%|ââââââââ  | 204M/268M [01:02<00:26, 2.41MB/s]\r",
      "Downloading:  76%|ââââââââ  | 205M/268M [01:02<00:24, 2.59MB/s]\r",
      "Downloading:  76%|ââââââââ  | 205M/268M [01:02<00:22, 2.84MB/s]\r",
      "Downloading:  77%|ââââââââ  | 205M/268M [01:02<00:25, 2.46MB/s]\r",
      "Downloading:  77%|ââââââââ  | 206M/268M [01:02<00:20, 3.00MB/s]\r",
      "Downloading:  77%|ââââââââ  | 206M/268M [01:02<00:19, 3.13MB/s]\r",
      "Downloading:  77%|ââââââââ  | 207M/268M [01:03<00:23, 2.63MB/s]\r",
      "Downloading:  77%|ââââââââ  | 207M/268M [01:03<00:22, 2.75MB/s]\r",
      "Downloading:  77%|ââââââââ  | 208M/268M [01:03<00:20, 2.98MB/s]\r",
      "Downloading:  78%|ââââââââ  | 208M/268M [01:03<00:16, 3.55MB/s]\r",
      "Downloading:  78%|ââââââââ  | 209M/268M [01:03<00:18, 3.12MB/s]\r",
      "Downloading:  78%|ââââââââ  | 209M/268M [01:03<00:18, 3.20MB/s]\r",
      "Downloading:  78%|ââââââââ  | 210M/268M [01:03<00:16, 3.61MB/s]\r",
      "Downloading:  78%|ââââââââ  | 210M/268M [01:04<00:19, 2.95MB/s]\r",
      "Downloading:  79%|ââââââââ  | 211M/268M [01:04<00:16, 3.48MB/s]\r",
      "Downloading:  79%|ââââââââ  | 211M/268M [01:04<00:23, 2.43MB/s]\r",
      "Downloading:  79%|ââââââââ  | 212M/268M [01:04<00:18, 3.06MB/s]\r",
      "Downloading:  79%|ââââââââ  | 212M/268M [01:04<00:17, 3.18MB/s]\r",
      "Downloading:  79%|ââââââââ  | 213M/268M [01:04<00:15, 3.51MB/s]\r",
      "Downloading:  80%|ââââââââ  | 213M/268M [01:05<00:15, 3.49MB/s]\r",
      "Downloading:  80%|ââââââââ  | 214M/268M [01:05<00:19, 2.85MB/s]\r",
      "Downloading:  80%|ââââââââ  | 214M/268M [01:05<00:18, 2.96MB/s]\r",
      "Downloading:  80%|ââââââââ  | 215M/268M [01:05<00:19, 2.74MB/s]\r",
      "Downloading:  80%|ââââââââ  | 215M/268M [01:05<00:17, 3.05MB/s]\r",
      "Downloading:  80%|ââââââââ  | 216M/268M [01:05<00:17, 2.94MB/s]\r",
      "Downloading:  81%|ââââââââ  | 216M/268M [01:05<00:14, 3.48MB/s]\r",
      "Downloading:  81%|ââââââââ  | 217M/268M [01:06<00:13, 3.88MB/s]\r",
      "Downloading:  81%|ââââââââ  | 217M/268M [01:06<00:18, 2.82MB/s]\r",
      "Downloading:  81%|ââââââââ  | 218M/268M [01:06<00:15, 3.18MB/s]\r",
      "Downloading:  81%|âââââââââ | 218M/268M [01:06<00:15, 3.20MB/s]\r",
      "Downloading:  82%|âââââââââ | 219M/268M [01:06<00:15, 3.28MB/s]\r",
      "Downloading:  82%|âââââââââ | 219M/268M [01:06<00:15, 3.22MB/s]\r",
      "Downloading:  82%|âââââââââ | 220M/268M [01:07<00:15, 3.09MB/s]\r",
      "Downloading:  82%|âââââââââ | 220M/268M [01:07<00:12, 3.70MB/s]\r",
      "Downloading:  82%|âââââââââ | 221M/268M [01:07<00:13, 3.60MB/s]\r",
      "Downloading:  83%|âââââââââ | 221M/268M [01:07<00:16, 2.77MB/s]\r",
      "Downloading:  83%|âââââââââ | 222M/268M [01:07<00:13, 3.38MB/s]\r",
      "Downloading:  83%|âââââââââ | 222M/268M [01:07<00:14, 3.23MB/s]\r",
      "Downloading:  83%|âââââââââ | 223M/268M [01:07<00:13, 3.45MB/s]\r",
      "Downloading:  83%|âââââââââ | 223M/268M [01:08<00:16, 2.81MB/s]\r",
      "Downloading:  83%|âââââââââ | 224M/268M [01:08<00:13, 3.35MB/s]\r",
      "Downloading:  84%|âââââââââ | 224M/268M [01:08<00:12, 3.42MB/s]\r",
      "Downloading:  84%|âââââââââ | 225M/268M [01:08<00:13, 3.27MB/s]\r",
      "Downloading:  84%|âââââââââ | 225M/268M [01:08<00:12, 3.47MB/s]\r",
      "Downloading:  84%|âââââââââ | 225M/268M [01:08<00:15, 2.78MB/s]\r",
      "Downloading:  84%|âââââââââ | 226M/268M [01:08<00:12, 3.36MB/s]\r",
      "Downloading:  85%|âââââââââ | 226M/268M [01:09<00:17, 2.40MB/s]\r",
      "Downloading:  85%|âââââââââ | 227M/268M [01:09<00:14, 2.88MB/s]\r",
      "Downloading:  85%|âââââââââ | 228M/268M [01:09<00:13, 3.10MB/s]\r",
      "Downloading:  85%|âââââââââ | 228M/268M [01:09<00:12, 3.19MB/s]\r",
      "Downloading:  85%|âââââââââ | 228M/268M [01:09<00:12, 3.10MB/s]\r",
      "Downloading:  85%|âââââââââ | 229M/268M [01:09<00:12, 3.24MB/s]\r",
      "Downloading:  86%|âââââââââ | 229M/268M [01:09<00:10, 3.84MB/s]\r",
      "Downloading:  86%|âââââââââ | 230M/268M [01:10<00:12, 3.05MB/s]\r",
      "Downloading:  86%|âââââââââ | 230M/268M [01:10<00:12, 3.07MB/s]\r",
      "Downloading:  86%|âââââââââ | 231M/268M [01:10<00:11, 3.12MB/s]\r",
      "Downloading:  86%|âââââââââ | 231M/268M [01:10<00:12, 2.84MB/s]\r",
      "Downloading:  86%|âââââââââ | 232M/268M [01:10<00:11, 3.21MB/s]\r",
      "Downloading:  87%|âââââââââ | 232M/268M [01:10<00:10, 3.36MB/s]\r",
      "Downloading:  87%|âââââââââ | 233M/268M [01:10<00:10, 3.26MB/s]\r",
      "Downloading:  87%|âââââââââ | 233M/268M [01:10<00:10, 3.32MB/s]\r",
      "Downloading:  87%|âââââââââ | 233M/268M [01:11<00:09, 3.46MB/s]\r",
      "Downloading:  87%|âââââââââ | 234M/268M [01:11<00:09, 3.39MB/s]\r",
      "Downloading:  88%|âââââââââ | 234M/268M [01:11<00:09, 3.48MB/s]\r",
      "Downloading:  88%|âââââââââ | 235M/268M [01:11<00:10, 3.04MB/s]\r",
      "Downloading:  88%|âââââââââ | 236M/268M [01:11<00:08, 3.70MB/s]\r",
      "Downloading:  88%|âââââââââ | 236M/268M [01:11<00:10, 3.05MB/s]\r",
      "Downloading:  88%|âââââââââ | 236M/268M [01:11<00:10, 3.04MB/s]\r",
      "Downloading:  88%|âââââââââ | 237M/268M [01:12<00:09, 3.16MB/s]\r",
      "Downloading:  89%|âââââââââ | 238M/268M [01:12<00:08, 3.75MB/s]\r",
      "Downloading:  89%|âââââââââ | 238M/268M [01:12<00:13, 2.20MB/s]\r",
      "Downloading:  89%|âââââââââ | 238M/268M [01:12<00:14, 2.11MB/s]\r",
      "Downloading:  89%|âââââââââ | 239M/268M [01:12<00:13, 2.12MB/s]\r",
      "Downloading:  89%|âââââââââ | 239M/268M [01:13<00:11, 2.55MB/s]\r",
      "Downloading:  89%|âââââââââ | 239M/268M [01:13<00:10, 2.74MB/s]\r",
      "Downloading:  89%|âââââââââ | 240M/268M [01:13<00:09, 2.91MB/s]\r",
      "Downloading:  90%|âââââââââ | 240M/268M [01:13<00:09, 3.02MB/s]\r",
      "Downloading:  90%|âââââââââ | 240M/268M [01:13<00:08, 3.15MB/s]\r",
      "Downloading:  90%|âââââââââ | 241M/268M [01:13<00:08, 3.22MB/s]\r",
      "Downloading:  90%|âââââââââ | 241M/268M [01:13<00:08, 3.28MB/s]\r",
      "Downloading:  90%|âââââââââ | 242M/268M [01:13<00:08, 3.30MB/s]\r",
      "Downloading:  90%|âââââââââ | 242M/268M [01:13<00:07, 3.31MB/s]\r",
      "Downloading:  90%|âââââââââ | 242M/268M [01:13<00:07, 3.35MB/s]\r",
      "Downloading:  91%|âââââââââ | 243M/268M [01:14<00:09, 2.65MB/s]\r",
      "Downloading:  91%|âââââââââ | 243M/268M [01:14<00:08, 2.82MB/s]\r",
      "Downloading:  91%|âââââââââ | 244M/268M [01:14<00:07, 3.07MB/s]\r",
      "Downloading:  91%|âââââââââ | 244M/268M [01:14<00:06, 3.61MB/s]\r",
      "Downloading:  91%|ââââââââââ| 245M/268M [01:14<00:08, 2.90MB/s]\r",
      "Downloading:  91%|ââââââââââ| 245M/268M [01:14<00:07, 3.01MB/s]\r",
      "Downloading:  92%|ââââââââââ| 246M/268M [01:15<00:07, 3.09MB/s]\r",
      "Downloading:  92%|ââââââââââ| 246M/268M [01:15<00:06, 3.26MB/s]\r",
      "Downloading:  92%|ââââââââââ| 247M/268M [01:15<00:05, 3.83MB/s]\r",
      "Downloading:  92%|ââââââââââ| 247M/268M [01:15<00:07, 2.97MB/s]\r",
      "Downloading:  92%|ââââââââââ| 248M/268M [01:15<00:05, 3.58MB/s]\r",
      "Downloading:  93%|ââââââââââ| 248M/268M [01:15<00:05, 3.55MB/s]\r",
      "Downloading:  93%|ââââââââââ| 249M/268M [01:15<00:06, 2.94MB/s]\r",
      "Downloading:  93%|ââââââââââ| 249M/268M [01:15<00:05, 3.48MB/s]\r",
      "Downloading:  93%|ââââââââââ| 250M/268M [01:16<00:05, 3.47MB/s]\r",
      "Downloading:  93%|ââââââââââ| 250M/268M [01:16<00:05, 3.06MB/s]\r",
      "Downloading:  94%|ââââââââââ| 251M/268M [01:16<00:05, 3.46MB/s]\r",
      "Downloading:  94%|ââââââââââ| 251M/268M [01:16<00:05, 2.89MB/s]\r",
      "Downloading:  94%|ââââââââââ| 252M/268M [01:16<00:04, 3.42MB/s]\r",
      "Downloading:  94%|ââââââââââ| 252M/268M [01:16<00:04, 3.44MB/s]\r",
      "Downloading:  94%|ââââââââââ| 252M/268M [01:16<00:04, 3.43MB/s]\r",
      "Downloading:  94%|ââââââââââ| 253M/268M [01:17<00:04, 3.45MB/s]\r",
      "Downloading:  94%|ââââââââââ| 253M/268M [01:17<00:04, 3.39MB/s]\r",
      "Downloading:  95%|ââââââââââ| 254M/268M [01:17<00:05, 2.71MB/s]\r",
      "Downloading:  95%|ââââââââââ| 254M/268M [01:17<00:04, 3.27MB/s]\r",
      "Downloading:  95%|ââââââââââ| 255M/268M [01:17<00:04, 3.31MB/s]\r",
      "Downloading:  95%|ââââââââââ| 255M/268M [01:17<00:03, 3.29MB/s]\r",
      "Downloading:  95%|ââââââââââ| 255M/268M [01:17<00:03, 3.28MB/s]\r",
      "Downloading:  95%|ââââââââââ| 256M/268M [01:17<00:04, 2.70MB/s]\r",
      "Downloading:  96%|ââââââââââ| 256M/268M [01:18<00:03, 3.19MB/s]\r",
      "Downloading:  96%|ââââââââââ| 257M/268M [01:18<00:03, 3.24MB/s]\r",
      "Downloading:  96%|ââââââââââ| 257M/268M [01:18<00:03, 3.20MB/s]\r",
      "Downloading:  96%|ââââââââââ| 257M/268M [01:18<00:03, 3.29MB/s]\r",
      "Downloading:  96%|ââââââââââ| 258M/268M [01:18<00:04, 2.34MB/s]\r",
      "Downloading:  96%|ââââââââââ| 258M/268M [01:18<00:03, 2.75MB/s]\r",
      "Downloading:  97%|ââââââââââ| 259M/268M [01:18<00:02, 3.28MB/s]\r",
      "Downloading:  97%|ââââââââââ| 259M/268M [01:19<00:03, 2.85MB/s]\r",
      "Downloading:  97%|ââââââââââ| 260M/268M [01:19<00:02, 3.18MB/s]\r",
      "Downloading:  97%|ââââââââââ| 260M/268M [01:19<00:02, 3.64MB/s]\r",
      "Downloading:  97%|ââââââââââ| 261M/268M [01:19<00:02, 2.97MB/s]\r",
      "Downloading:  98%|ââââââââââ| 261M/268M [01:19<00:01, 3.51MB/s]\r",
      "Downloading:  98%|ââââââââââ| 262M/268M [01:19<00:02, 2.88MB/s]\r",
      "Downloading:  98%|ââââââââââ| 262M/268M [01:19<00:01, 3.02MB/s]\r",
      "Downloading:  98%|ââââââââââ| 263M/268M [01:20<00:01, 3.09MB/s]\r",
      "Downloading:  98%|ââââââââââ| 263M/268M [01:20<00:01, 3.45MB/s]\r",
      "Downloading:  98%|ââââââââââ| 264M/268M [01:20<00:01, 3.47MB/s]\r",
      "Downloading:  99%|ââââââââââ| 264M/268M [01:20<00:01, 3.37MB/s]\r",
      "Downloading:  99%|ââââââââââ| 265M/268M [01:20<00:00, 3.79MB/s]\r",
      "Downloading:  99%|ââââââââââ| 265M/268M [01:20<00:00, 3.58MB/s]\r",
      "Downloading:  99%|ââââââââââ| 265M/268M [01:20<00:00, 3.65MB/s]\r",
      "Downloading:  99%|ââââââââââ| 266M/268M [01:20<00:00, 2.89MB/s]\r",
      "Downloading:  99%|ââââââââââ| 266M/268M [01:21<00:00, 2.84MB/s]\r",
      "Downloading: 100%|ââââââââââ| 267M/268M [01:21<00:00, 3.46MB/s]\r",
      "Downloading: 100%|ââââââââââ| 267M/268M [01:21<00:00, 3.04MB/s]\r",
      "Downloading: 100%|ââââââââââ| 268M/268M [01:21<00:00, 3.48MB/s]\r",
      "Downloading: 100%|ââââââââââ| 268M/268M [01:21<00:00, 3.29MB/s]\r\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\r\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\r\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "\u001b[36malgo-1-nck71_1  |\u001b[0m \r",
      "  0%|          | 0/63 [00:00<?, ?it/s]/codebuild/output/src708968123/src/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mtmp7zytaq8k_algo-1-nck71_1 exited with code 1\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run: ['docker-compose', '-f', '/private/var/folders/jj/dzns9hc55db1vmfsjvrh9n8m0000gp/T/tmp7zytaq8k/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process exited with code: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Process exited with code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-e5c8f8e04048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpytorch_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \"\"\"\n\u001b[1;32m   1417\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         self.model_artifacts = self.container.train(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         )\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# which contains the exit code and append the command line to it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Failed to run: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcompose_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompose_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run: ['docker-compose', '-f', '/private/var/folders/jj/dzns9hc55db1vmfsjvrh9n8m0000gp/T/tmp7zytaq8k/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1"
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Estimator\n",
    "\n",
    "You run PyTorch training scripts on SageMaker by creating PyTorch Estimators. SageMaker training of your script is invoked when you call fit on a PyTorch Estimator. The following code sample shows how you train a custom PyTorch script `train.py`, passing in three hyperparameters (`epochs`). We are not going to pass any data into sagemaker training job instead it will be downloaded in `train.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "pytorch_estimator = PyTorch(entry_point='train.py',\n",
    "                            source_dir='src',\n",
    "                            sagemaker_session=sess,\n",
    "#                            use_spot_instances=True,\n",
    "#                            max_wait=7200, # Seconds to wait for spot instances to become available\n",
    "                            base_job_name='huggingface',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version='1.6.0',\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-22 17:04:18 Starting - Starting the training job...\n",
      "2020-12-22 17:04:41 Starting - Launching requested ML instancesProfilerReport-1608656657: InProgress\n",
      "......\n",
      "2020-12-22 17:05:42 Starting - Preparing the instances for training......\n",
      "2020-12-22 17:06:48 Downloading - Downloading input data...\n",
      "2020-12-22 17:07:23 Training - Downloading the training image......\n",
      "2020-12-22 17:08:29 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-12-22 17:08:30,456 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-12-22 17:08:30,482 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-12-22 17:08:31,926 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-22 17:08:32,264 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting transformers\n",
      "  Downloading transformers-4.1.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (4.46.0)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (0.7)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.9.4\n",
      "  Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (2.24.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (20.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sklearn->-r requirements.txt (line 3)) (0.23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 2)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 2)) (0.17.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (2.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (2020.6.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 3)) (1.5.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn, sacremoses\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=f23284be949e2596440a4e41807643831598218ff3038d841128c579c65bab38\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=218ab1375ef4b30bb042e4485cc13094e0b66070fe450b810f010bc6e07d11df\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, sacremoses, tokenizers, filelock, transformers, sklearn\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.0.12 regex-2020.11.13 sacremoses-0.0.43 sklearn-0.0 tokenizers-0.9.4 transformers-4.1.1\u001b[0m\n",
      "\u001b[34m2020-12-22 17:08:39,347 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 32,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-2020-12-22-17-04-17-536\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-22-17-04-17-536/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-22-17-04-17-536/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-2020-12-22-17-04-17-536\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-22-17-04-17-536/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/train']\u001b[0m\n",
      "\u001b[34m2020-12-22 17:08:44,619 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\"\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train.py\", line 34, in <module>\n",
      "    train_dataset  = torch.load(os.path.join(args.training_dir, 'train_dataset.pt'))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/serialization.py\", line 585, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/serialization.py\", line 765, in _legacy_load\n",
      "    result = unpickler.load()\u001b[0m\n",
      "\u001b[34mModuleNotFoundError: No module named 'datasets'\u001b[0m\n",
      "\n",
      "2020-12-22 17:09:03 Uploading - Uploading generated training model\n",
      "2020-12-22 17:09:03 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-2020-12-22-17-04-17-536: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\"\nTraceback (most recent call last):\n  File \"train.py\", line 34, in <module>\n    train_dataset  = torch.load(os.path.join(args.training_dir, 'train_dataset.pt'))\n  File \"/opt/conda/lib/python3.6/site-packages/torch/serialization.py\", line 585, in load\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/serialization.py\", line 765, in _legacy_load\n    result = unpickler.load()\nModuleNotFoundError: No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e5c8f8e04048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpytorch_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3644\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3645\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3646\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3219\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FailureReason\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"(No reason provided)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3220\u001b[0m             \u001b[0mjob_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatus_key_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"JobStatus\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3221\u001b[0;31m             raise exceptions.UnexpectedStatusException(\n\u001b[0m\u001b[1;32m   3222\u001b[0m                 message=\"Error for {job_type} {job_name}: {status}. Reason: {reason}\".format(\n\u001b[1;32m   3223\u001b[0m                     \u001b[0mjob_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-2020-12-22-17-04-17-536: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\"\nTraceback (most recent call last):\n  File \"train.py\", line 34, in <module>\n    train_dataset  = torch.load(os.path.join(args.training_dir, 'train_dataset.pt'))\n  File \"/opt/conda/lib/python3.6/site-packages/torch/serialization.py\", line 585, in load\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/serialization.py\", line 765, in _legacy_load\n    result = unpickler.load()\nModuleNotFoundError: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
